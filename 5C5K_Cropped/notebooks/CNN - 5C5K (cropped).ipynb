{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3210,"status":"ok","timestamp":1650720476035,"user":{"displayName":"Hpone Myat Khine","userId":"13391990417136282456"},"user_tz":-480},"id":"BRbBDE7vsvlM"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from random import sample\n","import matplotlib.pyplot as plt\n","from IPython.core.pylabtools import figsize\n","import glob\n","import cv2\n","\n","from google.colab import drive\n","\n","import matplotlib.pyplot as plt\n","import keras as ks\n","from keras import layers\n","from keras.models import Sequential\n","import tensorflow as tf\n","from keras.backend import categorical_crossentropy\n","\n","from sklearn.metrics import accuracy_score\n","\n","import os"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19732,"status":"ok","timestamp":1650720495760,"user":{"displayName":"Hpone Myat Khine","userId":"13391990417136282456"},"user_tz":-480},"id":"bw9CW39as2z5","outputId":"cd61ff72-e58b-48b7-e72e-8ee5b02ce722"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["#drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"okjp0ObMs448","executionInfo":{"status":"ok","timestamp":1650720871633,"user_tz":-480,"elapsed":270427,"user":{"displayName":"Hpone Myat Khine","userId":"13391990417136282456"}},"outputId":"292f8694-5895-4500-cd05-96af654d86d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 25000 files belonging to 5 classes.\n","Found 27427 files belonging to 5 classes.\n","Found 34281 files belonging to 5 classes.\n"]}],"source":["image_size = (120, 120)\n","batch_size = 32\n","## /content/drive/MyDrive/BT5153 Applied ML Project /5 Classes 5k each for train (cropped)\n","train_directory = '/content/drive/MyDrive/Colab Notebooks/BT5153 Applied ML Project /5 Classes 5k each for train (cropped)/Training_cropped'\n","val_directory = '/content/drive/MyDrive/Colab Notebooks/BT5153 Applied ML Project /5 Classes 5k each for train (cropped)/Valid_cropped'\n","test_directory = '/content/drive/MyDrive/Colab Notebooks/BT5153 Applied ML Project /5 Classes 5k each for train (cropped)/Test_cropped'\n","\n","train_ds = tf.keras.preprocessing.image_dataset_from_directory(train_directory, label_mode='categorical', image_size=image_size, batch_size=batch_size)\n","val_ds = tf.keras.preprocessing.image_dataset_from_directory(val_directory, label_mode='categorical', image_size=image_size, batch_size=batch_size)\n","test_ds = tf.keras.preprocessing.image_dataset_from_directory(test_directory, label_mode='categorical', image_size=image_size, batch_size=batch_size)\n","\n","train_ds = train_ds.prefetch(buffer_size=batch_size)\n","val_ds = val_ds.prefetch(buffer_size=batch_size)\n","test_ds = test_ds.prefetch(buffer_size=batch_size)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"2JOXzXBptLwr","executionInfo":{"status":"ok","timestamp":1650720877653,"user_tz":-480,"elapsed":645,"user":{"displayName":"Hpone Myat Khine","userId":"13391990417136282456"}}},"outputs":[],"source":["def make_model(input_shape, num_classes):\n","    inputs = ks.Input(shape=input_shape)\n","    x = layers.Rescaling(1.0/255)(inputs)\n","    x = layers.Conv2D(32, 3, strides=2, padding='same')(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Activation('relu')(x)\n","    \n","    x = layers.Conv2D(64, 3, strides=2, padding='same')(x)\n","    x = layers.BatchNormalization()(x)\n","    x = layers.Activation('relu')(x)\n","    \n","    previous_block_activation = x\n","    \n","    for size in (128, 256, 512, 728):\n","        x = layers.Activation('relu')(x)\n","        x = layers.SeparableConv2D(size, 3, padding='same')(x)\n","        x = layers.BatchNormalization()(x)\n","        \n","        x = layers.Activation('relu')(x)\n","        x = layers.SeparableConv2D(size, 3, padding='same')(x)\n","        x = layers.BatchNormalization()(x)      \n","        \n","        x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n","        \n","        residual = layers.Conv2D(size, 1, strides=2, padding='same')(previous_block_activation)\n","        x = layers.add([x, residual])\n","        previous_block_activation = x\n","\n","    x = layers.Activation('relu')(x)\n","    x = layers.SeparableConv2D(1024, 3, padding='same')(x)\n","    x = layers.BatchNormalization()(x)\n","    \n","    x = layers.GlobalAveragePooling2D()(x)\n","    \n","    x = layers.Dropout(0.5)(x)\n","\n","    outputs = layers.Dense(num_classes, activation='softmax')(x)\n","        \n","    return ks.Model(inputs, outputs)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"LewcfGB5tQLq","executionInfo":{"status":"ok","timestamp":1650721044085,"user_tz":-480,"elapsed":554,"user":{"displayName":"Hpone Myat Khine","userId":"13391990417136282456"}}},"outputs":[],"source":["model = make_model(image_size+(3,), 5)\n","\n","epochs = 1000\n","saved_model = '/content/drive/MyDrive/Colab Notebooks/5153 weights/CNN_5C5K_(cropped)_{epoch}.h5'\n","callbacks = [ks.callbacks.ModelCheckpoint(saved_model,verbose=1, save_best_only=True), \n","             ks.callbacks.EarlyStopping(patience=20,verbose=1, restore_best_weights=True)]\n","\n","model.compile(optimizer='adam', loss='CategoricalCrossentropy', metrics=['accuracy'])"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"xl-s0lh3tQN5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650728745119,"user_tz":-480,"elapsed":7699094,"user":{"displayName":"Hpone Myat Khine","userId":"13391990417136282456"}},"outputId":"0787f3cb-c8a7-4c4a-b630-fe3c02666c3a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/1000\n","782/782 [==============================] - ETA: 0s - loss: 1.0893 - accuracy: 0.5701\n","Epoch 1: val_loss improved from inf to 1.68925, saving model to /content/drive/MyDrive/Colab Notebooks/5153 weights/CNN_5C5K_(cropped)_1.h5\n","782/782 [==============================] - 6636s 8s/step - loss: 1.0893 - accuracy: 0.5701 - val_loss: 1.6893 - val_accuracy: 0.4713\n","Epoch 2/1000\n","781/782 [============================>.] - ETA: 0s - loss: 0.8348 - accuracy: 0.6898\n","Epoch 2: val_loss did not improve from 1.68925\n","782/782 [==============================] - 46s 58ms/step - loss: 0.8347 - accuracy: 0.6898 - val_loss: 1.8254 - val_accuracy: 0.4684\n","Epoch 3/1000\n","781/782 [============================>.] - ETA: 0s - loss: 0.7033 - accuracy: 0.7426\n","Epoch 3: val_loss improved from 1.68925 to 1.44004, saving model to /content/drive/MyDrive/Colab Notebooks/5153 weights/CNN_5C5K_(cropped)_3.h5\n","782/782 [==============================] - 48s 62ms/step - loss: 0.7036 - accuracy: 0.7426 - val_loss: 1.4400 - val_accuracy: 0.5496\n","Epoch 4/1000\n","781/782 [============================>.] - ETA: 0s - loss: 0.6001 - accuracy: 0.7822\n","Epoch 4: val_loss improved from 1.44004 to 1.11007, saving model to /content/drive/MyDrive/Colab Notebooks/5153 weights/CNN_5C5K_(cropped)_4.h5\n","782/782 [==============================] - 46s 58ms/step - loss: 0.6000 - accuracy: 0.7823 - val_loss: 1.1101 - val_accuracy: 0.6382\n","Epoch 5/1000\n","781/782 [============================>.] - ETA: 0s - loss: 0.5283 - accuracy: 0.8111\n","Epoch 5: val_loss did not improve from 1.11007\n","782/782 [==============================] - 45s 57ms/step - loss: 0.5283 - accuracy: 0.8112 - val_loss: 1.2359 - val_accuracy: 0.5843\n","Epoch 6/1000\n","781/782 [============================>.] - ETA: 0s - loss: 0.4520 - accuracy: 0.8401\n","Epoch 6: val_loss did not improve from 1.11007\n","782/782 [==============================] - 45s 57ms/step - loss: 0.4519 - accuracy: 0.8402 - val_loss: 1.1877 - val_accuracy: 0.6520\n","Epoch 7/1000\n","780/782 [============================>.] - ETA: 0s - loss: 0.4013 - accuracy: 0.8580\n","Epoch 7: val_loss did not improve from 1.11007\n","782/782 [==============================] - 46s 59ms/step - loss: 0.4011 - accuracy: 0.8580 - val_loss: 1.5449 - val_accuracy: 0.5656\n","Epoch 8/1000\n","781/782 [============================>.] - ETA: 0s - loss: 0.3392 - accuracy: 0.8794\n","Epoch 8: val_loss did not improve from 1.11007\n","782/782 [==============================] - 45s 57ms/step - loss: 0.3392 - accuracy: 0.8794 - val_loss: 1.6074 - val_accuracy: 0.5773\n","Epoch 9/1000\n","781/782 [============================>.] - ETA: 0s - loss: 0.2975 - accuracy: 0.8941\n","Epoch 9: val_loss did not improve from 1.11007\n","782/782 [==============================] - 45s 58ms/step - loss: 0.2976 - accuracy: 0.8941 - val_loss: 1.9485 - val_accuracy: 0.5901\n","Epoch 10/1000\n","781/782 [============================>.] - ETA: 0s - loss: 0.2561 - accuracy: 0.9085\n","Epoch 10: val_loss did not improve from 1.11007\n","782/782 [==============================] - 45s 58ms/step - loss: 0.2566 - accuracy: 0.9084 - val_loss: 1.5792 - val_accuracy: 0.6256\n","Epoch 11/1000\n","781/782 [============================>.] - ETA: 0s - loss: 0.2277 - accuracy: 0.9188\n","Epoch 11: val_loss did not improve from 1.11007\n","782/782 [==============================] - 45s 58ms/step - loss: 0.2278 - accuracy: 0.9188 - val_loss: 2.3016 - val_accuracy: 0.5093\n","Epoch 12/1000\n","781/782 [============================>.] - ETA: 0s - loss: 0.2102 - accuracy: 0.9245\n","Epoch 12: val_loss did not improve from 1.11007\n","782/782 [==============================] - 45s 57ms/step - loss: 0.2102 - accuracy: 0.9244 - val_loss: 1.9890 - val_accuracy: 0.5950\n","Epoch 13/1000\n","781/782 [============================>.] - ETA: 0s - loss: 0.1864 - accuracy: 0.9337\n","Epoch 13: val_loss did not improve from 1.11007\n","782/782 [==============================] - 45s 58ms/step - loss: 0.1864 - accuracy: 0.9337 - val_loss: 1.6502 - val_accuracy: 0.6482\n","Epoch 14/1000\n","781/782 [============================>.] - ETA: 0s - loss: 0.1741 - accuracy: 0.9379\n","Epoch 14: val_loss did not improve from 1.11007\n","782/782 [==============================] - 46s 58ms/step - loss: 0.1743 - accuracy: 0.9379 - val_loss: 2.0144 - val_accuracy: 0.6090\n","Epoch 15/1000\n","780/782 [============================>.] - ETA: 0s - loss: 0.1623 - accuracy: 0.9431\n","Epoch 15: val_loss did not improve from 1.11007\n","782/782 [==============================] - 46s 59ms/step - loss: 0.1622 - accuracy: 0.9431 - val_loss: 1.9676 - val_accuracy: 0.6161\n","Epoch 16/1000\n","781/782 [============================>.] - ETA: 0s - loss: 0.1455 - accuracy: 0.9494\n","Epoch 16: val_loss did not improve from 1.11007\n","782/782 [==============================] - 46s 59ms/step - loss: 0.1457 - accuracy: 0.9493 - val_loss: 2.1250 - val_accuracy: 0.6035\n","Epoch 17/1000\n","781/782 [============================>.] - ETA: 0s - loss: 0.1439 - accuracy: 0.9479\n","Epoch 17: val_loss did not improve from 1.11007\n","782/782 [==============================] - 46s 59ms/step - loss: 0.1439 - accuracy: 0.9479 - val_loss: 3.7348 - val_accuracy: 0.4406\n","Epoch 18/1000\n","780/782 [============================>.] - ETA: 0s - loss: 0.1301 - accuracy: 0.9550\n","Epoch 18: val_loss did not improve from 1.11007\n","782/782 [==============================] - 46s 59ms/step - loss: 0.1303 - accuracy: 0.9550 - val_loss: 2.0021 - val_accuracy: 0.6278\n","Epoch 19/1000\n","781/782 [============================>.] - ETA: 0s - loss: 0.1308 - accuracy: 0.9528\n","Epoch 19: val_loss did not improve from 1.11007\n","782/782 [==============================] - 46s 59ms/step - loss: 0.1308 - accuracy: 0.9527 - val_loss: 2.2360 - val_accuracy: 0.6097\n","Epoch 20/1000\n","780/782 [============================>.] - ETA: 0s - loss: 0.1229 - accuracy: 0.9575\n","Epoch 20: val_loss did not improve from 1.11007\n","782/782 [==============================] - 47s 60ms/step - loss: 0.1229 - accuracy: 0.9574 - val_loss: 1.8909 - val_accuracy: 0.6449\n","Epoch 21/1000\n","781/782 [============================>.] - ETA: 0s - loss: 0.1197 - accuracy: 0.9580\n","Epoch 21: val_loss did not improve from 1.11007\n","782/782 [==============================] - 47s 60ms/step - loss: 0.1199 - accuracy: 0.9580 - val_loss: 1.8714 - val_accuracy: 0.6492\n","Epoch 22/1000\n","780/782 [============================>.] - ETA: 0s - loss: 0.1134 - accuracy: 0.9597\n","Epoch 22: val_loss did not improve from 1.11007\n","782/782 [==============================] - 48s 61ms/step - loss: 0.1136 - accuracy: 0.9596 - val_loss: 1.4804 - val_accuracy: 0.6626\n","Epoch 23/1000\n","780/782 [============================>.] - ETA: 0s - loss: 0.1034 - accuracy: 0.9639\n","Epoch 23: val_loss did not improve from 1.11007\n","782/782 [==============================] - 49s 62ms/step - loss: 0.1036 - accuracy: 0.9639 - val_loss: 2.7208 - val_accuracy: 0.5694\n","Epoch 24/1000\n","781/782 [============================>.] - ETA: 0s - loss: 0.1071 - accuracy: 0.9633\n","Epoch 24: val_loss did not improve from 1.11007\n","Restoring model weights from the end of the best epoch: 4.\n","782/782 [==============================] - 49s 62ms/step - loss: 0.1072 - accuracy: 0.9633 - val_loss: 2.0471 - val_accuracy: 0.6393\n","Epoch 24: early stopping\n"]}],"source":["history = model.fit(train_ds, epochs=epochs, callbacks=callbacks, validation_data=val_ds)"]},{"cell_type":"code","source":[""],"metadata":{"id":"bAO7Pfywqrhq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"jGOQXMj_DSsn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Y3HK2qbj8etJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Ifyz1_riyXoK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"0bSn5lhm1-Dm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"lcoXp9NpvdfH"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":9,"metadata":{"id":"HsxL0KJJygAu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650733175492,"user_tz":-480,"elapsed":4363017,"user":{"displayName":"Hpone Myat Khine","userId":"13391990417136282456"}},"outputId":"141c2a54-bf93-4f5c-9f41-73e273551c62"},"outputs":[{"output_type":"stream","name":"stdout","text":["1/1072\n","2/1072\n","3/1072\n","4/1072\n","5/1072\n","6/1072\n","7/1072\n","8/1072\n","9/1072\n","10/1072\n","11/1072\n","12/1072\n","13/1072\n","14/1072\n","15/1072\n","16/1072\n","17/1072\n","18/1072\n","19/1072\n","20/1072\n","21/1072\n","22/1072\n","23/1072\n","24/1072\n","25/1072\n","26/1072\n","27/1072\n","28/1072\n","29/1072\n","30/1072\n","31/1072\n","32/1072\n","33/1072\n","34/1072\n","35/1072\n","36/1072\n","37/1072\n","38/1072\n","39/1072\n","40/1072\n","41/1072\n","42/1072\n","43/1072\n","44/1072\n","45/1072\n","46/1072\n","47/1072\n","48/1072\n","49/1072\n","50/1072\n","51/1072\n","52/1072\n","53/1072\n","54/1072\n","55/1072\n","56/1072\n","57/1072\n","58/1072\n","59/1072\n","60/1072\n","61/1072\n","62/1072\n","63/1072\n","64/1072\n","65/1072\n","66/1072\n","67/1072\n","68/1072\n","69/1072\n","70/1072\n","71/1072\n","72/1072\n","73/1072\n","74/1072\n","75/1072\n","76/1072\n","77/1072\n","78/1072\n","79/1072\n","80/1072\n","81/1072\n","82/1072\n","83/1072\n","84/1072\n","85/1072\n","86/1072\n","87/1072\n","88/1072\n","89/1072\n","90/1072\n","91/1072\n","92/1072\n","93/1072\n","94/1072\n","95/1072\n","96/1072\n","97/1072\n","98/1072\n","99/1072\n","100/1072\n","101/1072\n","102/1072\n","103/1072\n","104/1072\n","105/1072\n","106/1072\n","107/1072\n","108/1072\n","109/1072\n","110/1072\n","111/1072\n","112/1072\n","113/1072\n","114/1072\n","115/1072\n","116/1072\n","117/1072\n","118/1072\n","119/1072\n","120/1072\n","121/1072\n","122/1072\n","123/1072\n","124/1072\n","125/1072\n","126/1072\n","127/1072\n","128/1072\n","129/1072\n","130/1072\n","131/1072\n","132/1072\n","133/1072\n","134/1072\n","135/1072\n","136/1072\n","137/1072\n","138/1072\n","139/1072\n","140/1072\n","141/1072\n","142/1072\n","143/1072\n","144/1072\n","145/1072\n","146/1072\n","147/1072\n","148/1072\n","149/1072\n","150/1072\n","151/1072\n","152/1072\n","153/1072\n","154/1072\n","155/1072\n","156/1072\n","157/1072\n","158/1072\n","159/1072\n","160/1072\n","161/1072\n","162/1072\n","163/1072\n","164/1072\n","165/1072\n","166/1072\n","167/1072\n","168/1072\n","169/1072\n","170/1072\n","171/1072\n","172/1072\n","173/1072\n","174/1072\n","175/1072\n","176/1072\n","177/1072\n","178/1072\n","179/1072\n","180/1072\n","181/1072\n","182/1072\n","183/1072\n","184/1072\n","185/1072\n","186/1072\n","187/1072\n","188/1072\n","189/1072\n","190/1072\n","191/1072\n","192/1072\n","193/1072\n","194/1072\n","195/1072\n","196/1072\n","197/1072\n","198/1072\n","199/1072\n","200/1072\n","201/1072\n","202/1072\n","203/1072\n","204/1072\n","205/1072\n","206/1072\n","207/1072\n","208/1072\n","209/1072\n","210/1072\n","211/1072\n","212/1072\n","213/1072\n","214/1072\n","215/1072\n","216/1072\n","217/1072\n","218/1072\n","219/1072\n","220/1072\n","221/1072\n","222/1072\n","223/1072\n","224/1072\n","225/1072\n","226/1072\n","227/1072\n","228/1072\n","229/1072\n","230/1072\n","231/1072\n","232/1072\n","233/1072\n","234/1072\n","235/1072\n","236/1072\n","237/1072\n","238/1072\n","239/1072\n","240/1072\n","241/1072\n","242/1072\n","243/1072\n","244/1072\n","245/1072\n","246/1072\n","247/1072\n","248/1072\n","249/1072\n","250/1072\n","251/1072\n","252/1072\n","253/1072\n","254/1072\n","255/1072\n","256/1072\n","257/1072\n","258/1072\n","259/1072\n","260/1072\n","261/1072\n","262/1072\n","263/1072\n","264/1072\n","265/1072\n","266/1072\n","267/1072\n","268/1072\n","269/1072\n","270/1072\n","271/1072\n","272/1072\n","273/1072\n","274/1072\n","275/1072\n","276/1072\n","277/1072\n","278/1072\n","279/1072\n","280/1072\n","281/1072\n","282/1072\n","283/1072\n","284/1072\n","285/1072\n","286/1072\n","287/1072\n","288/1072\n","289/1072\n","290/1072\n","291/1072\n","292/1072\n","293/1072\n","294/1072\n","295/1072\n","296/1072\n","297/1072\n","298/1072\n","299/1072\n","300/1072\n","301/1072\n","302/1072\n","303/1072\n","304/1072\n","305/1072\n","306/1072\n","307/1072\n","308/1072\n","309/1072\n","310/1072\n","311/1072\n","312/1072\n","313/1072\n","314/1072\n","315/1072\n","316/1072\n","317/1072\n","318/1072\n","319/1072\n","320/1072\n","321/1072\n","322/1072\n","323/1072\n","324/1072\n","325/1072\n","326/1072\n","327/1072\n","328/1072\n","329/1072\n","330/1072\n","331/1072\n","332/1072\n","333/1072\n","334/1072\n","335/1072\n","336/1072\n","337/1072\n","338/1072\n","339/1072\n","340/1072\n","341/1072\n","342/1072\n","343/1072\n","344/1072\n","345/1072\n","346/1072\n","347/1072\n","348/1072\n","349/1072\n","350/1072\n","351/1072\n","352/1072\n","353/1072\n","354/1072\n","355/1072\n","356/1072\n","357/1072\n","358/1072\n","359/1072\n","360/1072\n","361/1072\n","362/1072\n","363/1072\n","364/1072\n","365/1072\n","366/1072\n","367/1072\n","368/1072\n","369/1072\n","370/1072\n","371/1072\n","372/1072\n","373/1072\n","374/1072\n","375/1072\n","376/1072\n","377/1072\n","378/1072\n","379/1072\n","380/1072\n","381/1072\n","382/1072\n","383/1072\n","384/1072\n","385/1072\n","386/1072\n","387/1072\n","388/1072\n","389/1072\n","390/1072\n","391/1072\n","392/1072\n","393/1072\n","394/1072\n","395/1072\n","396/1072\n","397/1072\n","398/1072\n","399/1072\n","400/1072\n","401/1072\n","402/1072\n","403/1072\n","404/1072\n","405/1072\n","406/1072\n","407/1072\n","408/1072\n","409/1072\n","410/1072\n","411/1072\n","412/1072\n","413/1072\n","414/1072\n","415/1072\n","416/1072\n","417/1072\n","418/1072\n","419/1072\n","420/1072\n","421/1072\n","422/1072\n","423/1072\n","424/1072\n","425/1072\n","426/1072\n","427/1072\n","428/1072\n","429/1072\n","430/1072\n","431/1072\n","432/1072\n","433/1072\n","434/1072\n","435/1072\n","436/1072\n","437/1072\n","438/1072\n","439/1072\n","440/1072\n","441/1072\n","442/1072\n","443/1072\n","444/1072\n","445/1072\n","446/1072\n","447/1072\n","448/1072\n","449/1072\n","450/1072\n","451/1072\n","452/1072\n","453/1072\n","454/1072\n","455/1072\n","456/1072\n","457/1072\n","458/1072\n","459/1072\n","460/1072\n","461/1072\n","462/1072\n","463/1072\n","464/1072\n","465/1072\n","466/1072\n","467/1072\n","468/1072\n","469/1072\n","470/1072\n","471/1072\n","472/1072\n","473/1072\n","474/1072\n","475/1072\n","476/1072\n","477/1072\n","478/1072\n","479/1072\n","480/1072\n","481/1072\n","482/1072\n","483/1072\n","484/1072\n","485/1072\n","486/1072\n","487/1072\n","488/1072\n","489/1072\n","490/1072\n","491/1072\n","492/1072\n","493/1072\n","494/1072\n","495/1072\n","496/1072\n","497/1072\n","498/1072\n","499/1072\n","500/1072\n","501/1072\n","502/1072\n","503/1072\n","504/1072\n","505/1072\n","506/1072\n","507/1072\n","508/1072\n","509/1072\n","510/1072\n","511/1072\n","512/1072\n","513/1072\n","514/1072\n","515/1072\n","516/1072\n","517/1072\n","518/1072\n","519/1072\n","520/1072\n","521/1072\n","522/1072\n","523/1072\n","524/1072\n","525/1072\n","526/1072\n","527/1072\n","528/1072\n","529/1072\n","530/1072\n","531/1072\n","532/1072\n","533/1072\n","534/1072\n","535/1072\n","536/1072\n","537/1072\n","538/1072\n","539/1072\n","540/1072\n","541/1072\n","542/1072\n","543/1072\n","544/1072\n","545/1072\n","546/1072\n","547/1072\n","548/1072\n","549/1072\n","550/1072\n","551/1072\n","552/1072\n","553/1072\n","554/1072\n","555/1072\n","556/1072\n","557/1072\n","558/1072\n","559/1072\n","560/1072\n","561/1072\n","562/1072\n","563/1072\n","564/1072\n","565/1072\n","566/1072\n","567/1072\n","568/1072\n","569/1072\n","570/1072\n","571/1072\n","572/1072\n","573/1072\n","574/1072\n","575/1072\n","576/1072\n","577/1072\n","578/1072\n","579/1072\n","580/1072\n","581/1072\n","582/1072\n","583/1072\n","584/1072\n","585/1072\n","586/1072\n","587/1072\n","588/1072\n","589/1072\n","590/1072\n","591/1072\n","592/1072\n","593/1072\n","594/1072\n","595/1072\n","596/1072\n","597/1072\n","598/1072\n","599/1072\n","600/1072\n","601/1072\n","602/1072\n","603/1072\n","604/1072\n","605/1072\n","606/1072\n","607/1072\n","608/1072\n","609/1072\n","610/1072\n","611/1072\n","612/1072\n","613/1072\n","614/1072\n","615/1072\n","616/1072\n","617/1072\n","618/1072\n","619/1072\n","620/1072\n","621/1072\n","622/1072\n","623/1072\n","624/1072\n","625/1072\n","626/1072\n","627/1072\n","628/1072\n","629/1072\n","630/1072\n","631/1072\n","632/1072\n","633/1072\n","634/1072\n","635/1072\n","636/1072\n","637/1072\n","638/1072\n","639/1072\n","640/1072\n","641/1072\n","642/1072\n","643/1072\n","644/1072\n","645/1072\n","646/1072\n","647/1072\n","648/1072\n","649/1072\n","650/1072\n","651/1072\n","652/1072\n","653/1072\n","654/1072\n","655/1072\n","656/1072\n","657/1072\n","658/1072\n","659/1072\n","660/1072\n","661/1072\n","662/1072\n","663/1072\n","664/1072\n","665/1072\n","666/1072\n","667/1072\n","668/1072\n","669/1072\n","670/1072\n","671/1072\n","672/1072\n","673/1072\n","674/1072\n","675/1072\n","676/1072\n","677/1072\n","678/1072\n","679/1072\n","680/1072\n","681/1072\n","682/1072\n","683/1072\n","684/1072\n","685/1072\n","686/1072\n","687/1072\n","688/1072\n","689/1072\n","690/1072\n","691/1072\n","692/1072\n","693/1072\n","694/1072\n","695/1072\n","696/1072\n","697/1072\n","698/1072\n","699/1072\n","700/1072\n","701/1072\n","702/1072\n","703/1072\n","704/1072\n","705/1072\n","706/1072\n","707/1072\n","708/1072\n","709/1072\n","710/1072\n","711/1072\n","712/1072\n","713/1072\n","714/1072\n","715/1072\n","716/1072\n","717/1072\n","718/1072\n","719/1072\n","720/1072\n","721/1072\n","722/1072\n","723/1072\n","724/1072\n","725/1072\n","726/1072\n","727/1072\n","728/1072\n","729/1072\n","730/1072\n","731/1072\n","732/1072\n","733/1072\n","734/1072\n","735/1072\n","736/1072\n","737/1072\n","738/1072\n","739/1072\n","740/1072\n","741/1072\n","742/1072\n","743/1072\n","744/1072\n","745/1072\n","746/1072\n","747/1072\n","748/1072\n","749/1072\n","750/1072\n","751/1072\n","752/1072\n","753/1072\n","754/1072\n","755/1072\n","756/1072\n","757/1072\n","758/1072\n","759/1072\n","760/1072\n","761/1072\n","762/1072\n","763/1072\n","764/1072\n","765/1072\n","766/1072\n","767/1072\n","768/1072\n","769/1072\n","770/1072\n","771/1072\n","772/1072\n","773/1072\n","774/1072\n","775/1072\n","776/1072\n","777/1072\n","778/1072\n","779/1072\n","780/1072\n","781/1072\n","782/1072\n","783/1072\n","784/1072\n","785/1072\n","786/1072\n","787/1072\n","788/1072\n","789/1072\n","790/1072\n","791/1072\n","792/1072\n","793/1072\n","794/1072\n","795/1072\n","796/1072\n","797/1072\n","798/1072\n","799/1072\n","800/1072\n","801/1072\n","802/1072\n","803/1072\n","804/1072\n","805/1072\n","806/1072\n","807/1072\n","808/1072\n","809/1072\n","810/1072\n","811/1072\n","812/1072\n","813/1072\n","814/1072\n","815/1072\n","816/1072\n","817/1072\n","818/1072\n","819/1072\n","820/1072\n","821/1072\n","822/1072\n","823/1072\n","824/1072\n","825/1072\n","826/1072\n","827/1072\n","828/1072\n","829/1072\n","830/1072\n","831/1072\n","832/1072\n","833/1072\n","834/1072\n","835/1072\n","836/1072\n","837/1072\n","838/1072\n","839/1072\n","840/1072\n","841/1072\n","842/1072\n","843/1072\n","844/1072\n","845/1072\n","846/1072\n","847/1072\n","848/1072\n","849/1072\n","850/1072\n","851/1072\n","852/1072\n","853/1072\n","854/1072\n","855/1072\n","856/1072\n","857/1072\n","858/1072\n","859/1072\n","860/1072\n","861/1072\n","862/1072\n","863/1072\n","864/1072\n","865/1072\n","866/1072\n","867/1072\n","868/1072\n","869/1072\n","870/1072\n","871/1072\n","872/1072\n","873/1072\n","874/1072\n","875/1072\n","876/1072\n","877/1072\n","878/1072\n","879/1072\n","880/1072\n","881/1072\n","882/1072\n","883/1072\n","884/1072\n","885/1072\n","886/1072\n","887/1072\n","888/1072\n","889/1072\n","890/1072\n","891/1072\n","892/1072\n","893/1072\n","894/1072\n","895/1072\n","896/1072\n","897/1072\n","898/1072\n","899/1072\n","900/1072\n","901/1072\n","902/1072\n","903/1072\n","904/1072\n","905/1072\n","906/1072\n","907/1072\n","908/1072\n","909/1072\n","910/1072\n","911/1072\n","912/1072\n","913/1072\n","914/1072\n","915/1072\n","916/1072\n","917/1072\n","918/1072\n","919/1072\n","920/1072\n","921/1072\n","922/1072\n","923/1072\n","924/1072\n","925/1072\n","926/1072\n","927/1072\n","928/1072\n","929/1072\n","930/1072\n","931/1072\n","932/1072\n","933/1072\n","934/1072\n","935/1072\n","936/1072\n","937/1072\n","938/1072\n","939/1072\n","940/1072\n","941/1072\n","942/1072\n","943/1072\n","944/1072\n","945/1072\n","946/1072\n","947/1072\n","948/1072\n","949/1072\n","950/1072\n","951/1072\n","952/1072\n","953/1072\n","954/1072\n","955/1072\n","956/1072\n","957/1072\n","958/1072\n","959/1072\n","960/1072\n","961/1072\n","962/1072\n","963/1072\n","964/1072\n","965/1072\n","966/1072\n","967/1072\n","968/1072\n","969/1072\n","970/1072\n","971/1072\n","972/1072\n","973/1072\n","974/1072\n","975/1072\n","976/1072\n","977/1072\n","978/1072\n","979/1072\n","980/1072\n","981/1072\n","982/1072\n","983/1072\n","984/1072\n","985/1072\n","986/1072\n","987/1072\n","988/1072\n","989/1072\n","990/1072\n","991/1072\n","992/1072\n","993/1072\n","994/1072\n","995/1072\n","996/1072\n","997/1072\n","998/1072\n","999/1072\n","1000/1072\n","1001/1072\n","1002/1072\n","1003/1072\n","1004/1072\n","1005/1072\n","1006/1072\n","1007/1072\n","1008/1072\n","1009/1072\n","1010/1072\n","1011/1072\n","1012/1072\n","1013/1072\n","1014/1072\n","1015/1072\n","1016/1072\n","1017/1072\n","1018/1072\n","1019/1072\n","1020/1072\n","1021/1072\n","1022/1072\n","1023/1072\n","1024/1072\n","1025/1072\n","1026/1072\n","1027/1072\n","1028/1072\n","1029/1072\n","1030/1072\n","1031/1072\n","1032/1072\n","1033/1072\n","1034/1072\n","1035/1072\n","1036/1072\n","1037/1072\n","1038/1072\n","1039/1072\n","1040/1072\n","1041/1072\n","1042/1072\n","1043/1072\n","1044/1072\n","1045/1072\n","1046/1072\n","1047/1072\n","1048/1072\n","1049/1072\n","1050/1072\n","1051/1072\n","1052/1072\n","1053/1072\n","1054/1072\n","1055/1072\n","1056/1072\n","1057/1072\n","1058/1072\n","1059/1072\n","1060/1072\n","1061/1072\n","1062/1072\n","1063/1072\n","1064/1072\n","1065/1072\n","1066/1072\n","1067/1072\n","1068/1072\n","1069/1072\n","1070/1072\n","1071/1072\n","1072/1072\n"]}],"source":["true = []\n","pred = []\n","batches = int(np.ceil(34281/32))\n","i = 1\n","for image, label in test_ds:\n","  pred = np.concatenate([pred, \n","                         np.argmax(model.predict(image), axis=-1)])\n","  true = np.concatenate([true,\n","                         np.argmax(label.numpy(), axis=-1)])\n","  print('{}/{}'.format(i, batches))\n","  i += 1"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"BZN7ekLro8eB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650733177441,"user_tz":-480,"elapsed":12,"user":{"displayName":"Hpone Myat Khine","userId":"13391990417136282456"}},"outputId":"5ad3663a-4618-42d1-b51e-5c2a2fb65bd1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.6354248709197514"]},"metadata":{},"execution_count":10}],"source":["accuracy_score(true, pred)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"CWl1vq3ZoxQ7","executionInfo":{"status":"ok","timestamp":1650733177442,"user_tz":-480,"elapsed":7,"user":{"displayName":"Hpone Myat Khine","userId":"13391990417136282456"}}},"outputs":[],"source":["model_results = pd.DataFrame({'true':true,\n","                              'pred':pred})\n","\n","model_results.to_csv('/content/drive/MyDrive/Colab Notebooks/5153 weights/CNN_cropped_test_preds.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b0N946PypeZR"},"outputs":[],"source":[""]},{"cell_type":"code","source":[""],"metadata":{"id":"m5khcT8BPJge"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"C2nbdWVbTyJt"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"CNN - 5C5K (cropped).ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
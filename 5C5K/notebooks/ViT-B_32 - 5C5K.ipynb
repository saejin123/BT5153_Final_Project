{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"9C_m6qm0kuEm"},"outputs":[],"source":[""],"id":"9C_m6qm0kuEm"},{"cell_type":"code","execution_count":null,"metadata":{"id":"C5YrRncjM86U"},"outputs":[],"source":["#from google.colab import drive\n","#drive.mount('/content/drive')"],"id":"C5YrRncjM86U"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5433,"status":"ok","timestamp":1650615231221,"user":{"displayName":"Hpone Myat Khine","userId":"13391990417136282456"},"user_tz":-480},"id":"uoKaFGHSk27B","outputId":"e23039a9-0fd5-4906-dc13-cd83a8958cfe"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (0.16.1)\n","Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n"]}],"source":["!pip install -U tensorflow-addons\n","!pip install --quiet vit-keras"],"id":"uoKaFGHSk27B"},{"cell_type":"code","execution_count":null,"metadata":{"id":"b9855275"},"outputs":[],"source":["import os\n","import matplotlib.pyplot as plt\n","import keras as ks\n","from keras import layers\n","from keras.models import Sequential\n","import tensorflow as tf\n","import glob,warnings \n","warnings.filterwarnings('ignore')\n","\n","import pandas as pd \n","import numpy as np \n","import os \n","import tensorflow as tf\n","import cv2\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import Dense, Input, InputLayer, Flatten\n","from tensorflow.keras.models import Sequential, Model\n","import matplotlib.pyplot as plt \n","import seaborn as sns\n","import matplotlib.image as mpimg\n","%matplotlib inline\n","\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import tensorflow_addons as tfa\n","\n","from sklearn.metrics import confusion_matrix, classification_report\n","\n","\n"],"id":"b9855275"},{"cell_type":"code","execution_count":null,"metadata":{"id":"bi3ui6HClEX7"},"outputs":[],"source":["#IMG_WIDTH = 72\n","#IMG_HEIGHT = 72\n","\n","learning_rate = 0.001\n","weight_decay = 0.0001\n","batch_size = 32\n","num_epochs = 100\n","image_size = 128  # We'll resize input images to this size\n","image_size_read = (128,128)\n","patch_size = 6  # Size of the patches to be extract from the input images\n","num_patches = (image_size // patch_size) ** 2\n","projection_dim = 64\n","num_heads = 4\n","transformer_units = [\n","    projection_dim * 2,\n","    projection_dim,\n","]  # Size of the transformer layers\n","transformer_layers = 8\n","mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier"],"id":"bi3ui6HClEX7"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4034,"status":"ok","timestamp":1650642701842,"user":{"displayName":"Hpone Myat Khine","userId":"13391990417136282456"},"user_tz":-480},"id":"fBcVDNUoiO0O","outputId":"e3537434-0975-42ce-b2fb-774e52d2c0dd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 25000 files belonging to 5 classes.\n","Found 27447 files belonging to 5 classes.\n","Found 34281 files belonging to 5 classes.\n"]}],"source":["\n","## try from here\n","train_directory = '/content/drive/MyDrive/Colab Notebooks/BT5153 Applied ML Project /5 classes 5k each for train/Training'\n","val_directory = '/content/drive/MyDrive/Colab Notebooks/BT5153 Applied ML Project /5 classes 5k each for train/Validation'\n","test_directory = '/content/drive/MyDrive/Colab Notebooks/BT5153 Applied ML Project /5 classes 5k each for train/Testing'\n","\n","train_ds = tf.keras.preprocessing.image_dataset_from_directory(train_directory, label_mode='categorical', image_size=image_size_read, batch_size=batch_size)\n","val_ds = tf.keras.preprocessing.image_dataset_from_directory(val_directory, label_mode='categorical', image_size=image_size_read, batch_size=batch_size)\n","test_ds = tf.keras.preprocessing.image_dataset_from_directory(test_directory, label_mode='categorical', image_size=image_size_read, batch_size=batch_size)\n","\n","#train_ds = train_ds.prefetch(buffer_size=128)\n","#val_ds = val_ds.prefetch(buffer_size=128)\n","#test_ds = test_ds.prefetch(buffer_size=128)"],"id":"fBcVDNUoiO0O"},{"cell_type":"code","execution_count":null,"metadata":{"id":"G66yutAYicad"},"outputs":[],"source":[""],"id":"G66yutAYicad"},{"cell_type":"code","execution_count":null,"metadata":{"id":"MtbHd6aio2tt"},"outputs":[],"source":["#data_augmentation = keras.Sequential(\n","#    [\n","#        layers.Normalization(),\n","#        layers.Resizing(image_size, image_size),\n","#        layers.RandomFlip(\"horizontal\"),\n","#        layers.RandomRotation(factor=0.02),\n","#        layers.RandomZoom(\n","#            height_factor=0.2, width_factor=0.2\n","#        ),\n","#    ],\n","#    name=\"data_augmentation\",\n","#)\n","# Compute the mean and the variance of the training data for normalization.\n","#data_augmentation.layers[0].adapt(X_train)\n"],"id":"MtbHd6aio2tt"},{"cell_type":"code","execution_count":null,"metadata":{"id":"mnQnteUjTOSA"},"outputs":[],"source":[""],"id":"mnQnteUjTOSA"},{"cell_type":"markdown","metadata":{"id":"vogXv6_hTPan"},"source":["## Build the model"],"id":"vogXv6_hTPan"},{"cell_type":"code","execution_count":null,"metadata":{"id":"yyKMc9jQTQwv"},"outputs":[],"source":["from vit_keras import vit"],"id":"yyKMc9jQTQwv"},{"cell_type":"markdown","metadata":{"id":"EuWJ6HdFTXFa"},"source":["### ViT B32 Model"],"id":"EuWJ6HdFTXFa"},{"cell_type":"code","execution_count":null,"metadata":{"id":"XfMlRhB4TZOe"},"outputs":[],"source":["vit_model = vit.vit_b32(\n","        image_size = image_size,\n","        activation = 'softmax',\n","        pretrained = True,\n","        include_top = False,\n","        pretrained_top = False, #do not include the classifier at the top \n","        classes = 5) #5 class for ours "],"id":"XfMlRhB4TZOe"},{"cell_type":"code","source":["# Freeze the base_model\n","#vit_model.trainable = False"],"metadata":{"id":"cXXqoTMveG2t"},"id":"cXXqoTMveG2t","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O2AOPiA_TfBK"},"source":["#### Fine-tune"],"id":"O2AOPiA_TfBK"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1608,"status":"ok","timestamp":1650642714055,"user":{"displayName":"Hpone Myat Khine","userId":"13391990417136282456"},"user_tz":-480},"id":"sHJmCTYKTghg","outputId":"df58fda2-10f3-4b17-8404-caed2adf90f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"vision_transformer\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," vit-b32 (Functional)        (None, 768)               87429888  \n","                                                                 \n"," flatten_1 (Flatten)         (None, 768)               0         \n","                                                                 \n"," batch_normalization_2 (Batc  (None, 768)              3072      \n"," hNormalization)                                                 \n","                                                                 \n"," dense_2 (Dense)             (None, 11)                8459      \n","                                                                 \n"," batch_normalization_3 (Batc  (None, 11)               44        \n"," hNormalization)                                                 \n","                                                                 \n"," dense_3 (Dense)             (None, 5)                 60        \n","                                                                 \n","=================================================================\n","Total params: 87,441,523\n","Trainable params: 87,439,965\n","Non-trainable params: 1,558\n","_________________________________________________________________\n"]}],"source":["model = tf.keras.Sequential([\n","        vit_model,\n","        tf.keras.layers.Flatten(),\n","        tf.keras.layers.BatchNormalization(),\n","        tf.keras.layers.Dense(11, activation = tfa.activations.gelu),\n","        tf.keras.layers.BatchNormalization(),\n","        tf.keras.layers.Dense(5, 'softmax') #5 classes\n","    ],\n","    name = 'vision_transformer')\n","\n","model.summary()"],"id":"sHJmCTYKTghg"},{"cell_type":"code","source":["optimizer = tfa.optimizers.RectifiedAdam(learning_rate = learning_rate)\n","\n","model.compile(optimizer = optimizer, \n","              loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True), \n","              metrics = ['accuracy',\n","                         tfa.metrics.MatthewsCorrelationCoefficient(num_classes=5,name='mcc')])"],"metadata":{"id":"R0f3gXWwQcTN"},"id":"R0f3gXWwQcTN","execution_count":null,"outputs":[]},{"cell_type":"code","source":["earlystopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss',\n","                                                 patience = 10,\n","                                                 mode = 'min',\n","                                                 restore_best_weights = True,\n","                                                 verbose = 1)\n","\n","reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss',\n","                                                 factor = 0.3,\n","                                                 patience = 3,\n","                                                 verbose = 1,\n","                                                 min_delta = 1e-4,\n","                                                 mode = 'min')\n","\n","checkpoint_filepath = '/content/drive/MyDrive/Colab Notebooks/5153 weights/batchsize32_ViT_B-32_5C5K_{epoch}.h5' #/tmp/checkpoint #\n","checkpoint_callback = keras.callbacks.ModelCheckpoint(\n","        checkpoint_filepath,\n","        monitor=\"val_loss\",\n","        save_best_only=True,\n","       # save_weights_only=True, #don't save weights, save model \n","    )\n","\n","callbacks = [earlystopping, reduce_lr, checkpoint_callback]"],"metadata":{"id":"vVDIVafzQgmc"},"id":"vVDIVafzQgmc","execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.metrics_names"],"metadata":{"id":"K7eFyVWvQ7OF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650642727970,"user_tz":-480,"elapsed":14,"user":{"displayName":"Hpone Myat Khine","userId":"13391990417136282456"}},"outputId":"676bbeb6-087e-4db0-993a-333fd475235c"},"id":"K7eFyVWvQ7OF","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[]"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["model.fit(\n","        train_ds,\n","        #x=X_train,\n","        #y=y_train,\n","        batch_size=batch_size,\n","        epochs=num_epochs,\n","        validation_data=val_ds,\n","       # steps_per_epoch = 328,\n","       # validation_steps= 431,\n","        #validation_split=0.1,\n","        verbose=1,\n","        callbacks = callbacks)\n","\n"],"metadata":{"id":"LcV99uxSQwfk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650647582241,"user_tz":-480,"elapsed":4851875,"user":{"displayName":"Hpone Myat Khine","userId":"13391990417136282456"}},"outputId":"209798a6-c7e2-4c17-99fd-c0f64c1c0706"},"id":"LcV99uxSQwfk","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","782/782 [==============================] - 239s 273ms/step - loss: 1.4082 - accuracy: 0.4071 - mcc: 0.2601 - val_loss: 1.8320 - val_accuracy: 0.2681 - val_mcc: 0.1193 - lr: 0.0010\n","Epoch 2/100\n","782/782 [==============================] - 207s 265ms/step - loss: 1.1717 - accuracy: 0.5255 - mcc: 0.4079 - val_loss: 2.1102 - val_accuracy: 0.1831 - val_mcc: 0.0804 - lr: 0.0010\n","Epoch 3/100\n","782/782 [==============================] - 211s 270ms/step - loss: 1.1004 - accuracy: 0.5603 - mcc: 0.4509 - val_loss: 1.7493 - val_accuracy: 0.2519 - val_mcc: 0.1367 - lr: 0.0010\n","Epoch 4/100\n","782/782 [==============================] - 206s 264ms/step - loss: 1.0554 - accuracy: 0.5787 - mcc: 0.4738 - val_loss: 2.3820 - val_accuracy: 0.3460 - val_mcc: 0.2486 - lr: 0.0010\n","Epoch 5/100\n","782/782 [==============================] - 206s 263ms/step - loss: 1.0381 - accuracy: 0.5870 - mcc: 0.4840 - val_loss: 2.4534 - val_accuracy: 0.4078 - val_mcc: 0.1231 - lr: 0.0010\n","Epoch 6/100\n","782/782 [==============================] - 210s 269ms/step - loss: 1.0198 - accuracy: 0.5961 - mcc: 0.4954 - val_loss: 1.4256 - val_accuracy: 0.5299 - val_mcc: 0.3150 - lr: 0.0010\n","Epoch 7/100\n","782/782 [==============================] - 206s 264ms/step - loss: 0.9964 - accuracy: 0.6078 - mcc: 0.5100 - val_loss: 1.6778 - val_accuracy: 0.4662 - val_mcc: 0.3213 - lr: 0.0010\n","Epoch 8/100\n","782/782 [==============================] - 206s 263ms/step - loss: 0.9706 - accuracy: 0.6199 - mcc: 0.5251 - val_loss: 6.2747 - val_accuracy: 0.1830 - val_mcc: 0.0260 - lr: 0.0010\n","Epoch 9/100\n","782/782 [==============================] - ETA: 0s - loss: 0.9624 - accuracy: 0.6228 - mcc: 0.5288\n","Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n","782/782 [==============================] - 207s 264ms/step - loss: 0.9624 - accuracy: 0.6228 - mcc: 0.5288 - val_loss: 3.5138 - val_accuracy: 0.1916 - val_mcc: 0.1287 - lr: 0.0010\n","Epoch 10/100\n","782/782 [==============================] - 210s 268ms/step - loss: 0.8416 - accuracy: 0.6792 - mcc: 0.5993 - val_loss: 1.1947 - val_accuracy: 0.5563 - val_mcc: 0.4061 - lr: 3.0000e-04\n","Epoch 11/100\n","782/782 [==============================] - 210s 268ms/step - loss: 0.7903 - accuracy: 0.6993 - mcc: 0.6243 - val_loss: 1.1860 - val_accuracy: 0.5506 - val_mcc: 0.4396 - lr: 3.0000e-04\n","Epoch 12/100\n","782/782 [==============================] - 207s 265ms/step - loss: 0.7480 - accuracy: 0.7194 - mcc: 0.6494 - val_loss: 1.3174 - val_accuracy: 0.4909 - val_mcc: 0.3761 - lr: 3.0000e-04\n","Epoch 13/100\n","782/782 [==============================] - 210s 268ms/step - loss: 0.7054 - accuracy: 0.7352 - mcc: 0.6691 - val_loss: 1.0971 - val_accuracy: 0.5799 - val_mcc: 0.4333 - lr: 3.0000e-04\n","Epoch 14/100\n","782/782 [==============================] - 207s 265ms/step - loss: 0.6677 - accuracy: 0.7530 - mcc: 0.6913 - val_loss: 1.2419 - val_accuracy: 0.5679 - val_mcc: 0.4088 - lr: 3.0000e-04\n","Epoch 15/100\n","782/782 [==============================] - 206s 263ms/step - loss: 0.6222 - accuracy: 0.7694 - mcc: 0.7118 - val_loss: 1.3685 - val_accuracy: 0.5256 - val_mcc: 0.3853 - lr: 3.0000e-04\n","Epoch 16/100\n","782/782 [==============================] - ETA: 0s - loss: 0.5924 - accuracy: 0.7818 - mcc: 0.7273\n","Epoch 16: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n","782/782 [==============================] - 206s 263ms/step - loss: 0.5924 - accuracy: 0.7818 - mcc: 0.7273 - val_loss: 1.2273 - val_accuracy: 0.5727 - val_mcc: 0.4432 - lr: 3.0000e-04\n","Epoch 17/100\n","782/782 [==============================] - 207s 265ms/step - loss: 0.4855 - accuracy: 0.8288 - mcc: 0.7861 - val_loss: 1.1695 - val_accuracy: 0.5878 - val_mcc: 0.4687 - lr: 9.0000e-05\n","Epoch 18/100\n","782/782 [==============================] - 206s 263ms/step - loss: 0.4257 - accuracy: 0.8516 - mcc: 0.8145 - val_loss: 1.2206 - val_accuracy: 0.5848 - val_mcc: 0.4687 - lr: 9.0000e-05\n","Epoch 19/100\n","782/782 [==============================] - ETA: 0s - loss: 0.3916 - accuracy: 0.8632 - mcc: 0.8290\n","Epoch 19: ReduceLROnPlateau reducing learning rate to 2.700000040931627e-05.\n","782/782 [==============================] - 206s 264ms/step - loss: 0.3916 - accuracy: 0.8632 - mcc: 0.8290 - val_loss: 1.2866 - val_accuracy: 0.5877 - val_mcc: 0.4713 - lr: 9.0000e-05\n","Epoch 20/100\n","782/782 [==============================] - 206s 263ms/step - loss: 0.3473 - accuracy: 0.8818 - mcc: 0.8523 - val_loss: 1.3082 - val_accuracy: 0.5905 - val_mcc: 0.4768 - lr: 2.7000e-05\n","Epoch 21/100\n","782/782 [==============================] - 207s 264ms/step - loss: 0.3173 - accuracy: 0.8942 - mcc: 0.8678 - val_loss: 1.3055 - val_accuracy: 0.5955 - val_mcc: 0.4799 - lr: 2.7000e-05\n","Epoch 22/100\n","782/782 [==============================] - ETA: 0s - loss: 0.3017 - accuracy: 0.9017 - mcc: 0.8771\n","Epoch 22: ReduceLROnPlateau reducing learning rate to 8.100000013655517e-06.\n","782/782 [==============================] - 207s 264ms/step - loss: 0.3017 - accuracy: 0.9017 - mcc: 0.8771 - val_loss: 1.3160 - val_accuracy: 0.5990 - val_mcc: 0.4788 - lr: 2.7000e-05\n","Epoch 23/100\n","782/782 [==============================] - ETA: 0s - loss: 0.2916 - accuracy: 0.9020 - mcc: 0.8775Restoring model weights from the end of the best epoch: 13.\n","782/782 [==============================] - 206s 263ms/step - loss: 0.2916 - accuracy: 0.9020 - mcc: 0.8775 - val_loss: 1.3579 - val_accuracy: 0.6000 - val_mcc: 0.4836 - lr: 8.1000e-06\n","Epoch 23: early stopping\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f9b6bf46250>"]},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":["test_loss, test_accuracy, test_mcc = model.evaluate(test_ds)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P4HDfWV68tEb","executionInfo":{"status":"ok","timestamp":1650647714861,"user_tz":-480,"elapsed":72322,"user":{"displayName":"Hpone Myat Khine","userId":"13391990417136282456"}},"outputId":"990090ff-6a6b-44dc-d173-2fe1957154fa"},"id":"P4HDfWV68tEb","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1072/1072 [==============================] - 72s 67ms/step - loss: 1.1001 - accuracy: 0.5789 - mcc: 0.4308\n"]}]},{"cell_type":"code","source":["print(f'Accuracy is {round(test_accuracy * 100, 2)}%')"],"metadata":{"id":"eogbDDliGhXl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650647714864,"user_tz":-480,"elapsed":50,"user":{"displayName":"Hpone Myat Khine","userId":"13391990417136282456"}},"outputId":"5b4f3eb6-2e27-4afc-cd90-3f6312fe1a9b"},"id":"eogbDDliGhXl","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy is 57.89%\n"]}]},{"cell_type":"code","source":["true = []\n","pred = []\n","batches = int(np.ceil(34281/32)) #34281 -- > test size / 32  is batch size \n","i = 1\n","for image, label in test_ds:\n","  pred = np.concatenate([pred, \n","                         np.argmax(model.predict(image), axis=-1)])\n","  true = np.concatenate([true,\n","                         np.argmax(label.numpy(), axis=-1)])\n","  print('{}/{}'.format(i, batches))\n","  i += 1"],"metadata":{"id":"PQgmd3FoGD9N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650647817351,"user_tz":-480,"elapsed":94833,"user":{"displayName":"Hpone Myat Khine","userId":"13391990417136282456"}},"outputId":"3ac86046-085a-4f16-dfd0-2f192e722b5a"},"id":"PQgmd3FoGD9N","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1072\n","2/1072\n","3/1072\n","4/1072\n","5/1072\n","6/1072\n","7/1072\n","8/1072\n","9/1072\n","10/1072\n","11/1072\n","12/1072\n","13/1072\n","14/1072\n","15/1072\n","16/1072\n","17/1072\n","18/1072\n","19/1072\n","20/1072\n","21/1072\n","22/1072\n","23/1072\n","24/1072\n","25/1072\n","26/1072\n","27/1072\n","28/1072\n","29/1072\n","30/1072\n","31/1072\n","32/1072\n","33/1072\n","34/1072\n","35/1072\n","36/1072\n","37/1072\n","38/1072\n","39/1072\n","40/1072\n","41/1072\n","42/1072\n","43/1072\n","44/1072\n","45/1072\n","46/1072\n","47/1072\n","48/1072\n","49/1072\n","50/1072\n","51/1072\n","52/1072\n","53/1072\n","54/1072\n","55/1072\n","56/1072\n","57/1072\n","58/1072\n","59/1072\n","60/1072\n","61/1072\n","62/1072\n","63/1072\n","64/1072\n","65/1072\n","66/1072\n","67/1072\n","68/1072\n","69/1072\n","70/1072\n","71/1072\n","72/1072\n","73/1072\n","74/1072\n","75/1072\n","76/1072\n","77/1072\n","78/1072\n","79/1072\n","80/1072\n","81/1072\n","82/1072\n","83/1072\n","84/1072\n","85/1072\n","86/1072\n","87/1072\n","88/1072\n","89/1072\n","90/1072\n","91/1072\n","92/1072\n","93/1072\n","94/1072\n","95/1072\n","96/1072\n","97/1072\n","98/1072\n","99/1072\n","100/1072\n","101/1072\n","102/1072\n","103/1072\n","104/1072\n","105/1072\n","106/1072\n","107/1072\n","108/1072\n","109/1072\n","110/1072\n","111/1072\n","112/1072\n","113/1072\n","114/1072\n","115/1072\n","116/1072\n","117/1072\n","118/1072\n","119/1072\n","120/1072\n","121/1072\n","122/1072\n","123/1072\n","124/1072\n","125/1072\n","126/1072\n","127/1072\n","128/1072\n","129/1072\n","130/1072\n","131/1072\n","132/1072\n","133/1072\n","134/1072\n","135/1072\n","136/1072\n","137/1072\n","138/1072\n","139/1072\n","140/1072\n","141/1072\n","142/1072\n","143/1072\n","144/1072\n","145/1072\n","146/1072\n","147/1072\n","148/1072\n","149/1072\n","150/1072\n","151/1072\n","152/1072\n","153/1072\n","154/1072\n","155/1072\n","156/1072\n","157/1072\n","158/1072\n","159/1072\n","160/1072\n","161/1072\n","162/1072\n","163/1072\n","164/1072\n","165/1072\n","166/1072\n","167/1072\n","168/1072\n","169/1072\n","170/1072\n","171/1072\n","172/1072\n","173/1072\n","174/1072\n","175/1072\n","176/1072\n","177/1072\n","178/1072\n","179/1072\n","180/1072\n","181/1072\n","182/1072\n","183/1072\n","184/1072\n","185/1072\n","186/1072\n","187/1072\n","188/1072\n","189/1072\n","190/1072\n","191/1072\n","192/1072\n","193/1072\n","194/1072\n","195/1072\n","196/1072\n","197/1072\n","198/1072\n","199/1072\n","200/1072\n","201/1072\n","202/1072\n","203/1072\n","204/1072\n","205/1072\n","206/1072\n","207/1072\n","208/1072\n","209/1072\n","210/1072\n","211/1072\n","212/1072\n","213/1072\n","214/1072\n","215/1072\n","216/1072\n","217/1072\n","218/1072\n","219/1072\n","220/1072\n","221/1072\n","222/1072\n","223/1072\n","224/1072\n","225/1072\n","226/1072\n","227/1072\n","228/1072\n","229/1072\n","230/1072\n","231/1072\n","232/1072\n","233/1072\n","234/1072\n","235/1072\n","236/1072\n","237/1072\n","238/1072\n","239/1072\n","240/1072\n","241/1072\n","242/1072\n","243/1072\n","244/1072\n","245/1072\n","246/1072\n","247/1072\n","248/1072\n","249/1072\n","250/1072\n","251/1072\n","252/1072\n","253/1072\n","254/1072\n","255/1072\n","256/1072\n","257/1072\n","258/1072\n","259/1072\n","260/1072\n","261/1072\n","262/1072\n","263/1072\n","264/1072\n","265/1072\n","266/1072\n","267/1072\n","268/1072\n","269/1072\n","270/1072\n","271/1072\n","272/1072\n","273/1072\n","274/1072\n","275/1072\n","276/1072\n","277/1072\n","278/1072\n","279/1072\n","280/1072\n","281/1072\n","282/1072\n","283/1072\n","284/1072\n","285/1072\n","286/1072\n","287/1072\n","288/1072\n","289/1072\n","290/1072\n","291/1072\n","292/1072\n","293/1072\n","294/1072\n","295/1072\n","296/1072\n","297/1072\n","298/1072\n","299/1072\n","300/1072\n","301/1072\n","302/1072\n","303/1072\n","304/1072\n","305/1072\n","306/1072\n","307/1072\n","308/1072\n","309/1072\n","310/1072\n","311/1072\n","312/1072\n","313/1072\n","314/1072\n","315/1072\n","316/1072\n","317/1072\n","318/1072\n","319/1072\n","320/1072\n","321/1072\n","322/1072\n","323/1072\n","324/1072\n","325/1072\n","326/1072\n","327/1072\n","328/1072\n","329/1072\n","330/1072\n","331/1072\n","332/1072\n","333/1072\n","334/1072\n","335/1072\n","336/1072\n","337/1072\n","338/1072\n","339/1072\n","340/1072\n","341/1072\n","342/1072\n","343/1072\n","344/1072\n","345/1072\n","346/1072\n","347/1072\n","348/1072\n","349/1072\n","350/1072\n","351/1072\n","352/1072\n","353/1072\n","354/1072\n","355/1072\n","356/1072\n","357/1072\n","358/1072\n","359/1072\n","360/1072\n","361/1072\n","362/1072\n","363/1072\n","364/1072\n","365/1072\n","366/1072\n","367/1072\n","368/1072\n","369/1072\n","370/1072\n","371/1072\n","372/1072\n","373/1072\n","374/1072\n","375/1072\n","376/1072\n","377/1072\n","378/1072\n","379/1072\n","380/1072\n","381/1072\n","382/1072\n","383/1072\n","384/1072\n","385/1072\n","386/1072\n","387/1072\n","388/1072\n","389/1072\n","390/1072\n","391/1072\n","392/1072\n","393/1072\n","394/1072\n","395/1072\n","396/1072\n","397/1072\n","398/1072\n","399/1072\n","400/1072\n","401/1072\n","402/1072\n","403/1072\n","404/1072\n","405/1072\n","406/1072\n","407/1072\n","408/1072\n","409/1072\n","410/1072\n","411/1072\n","412/1072\n","413/1072\n","414/1072\n","415/1072\n","416/1072\n","417/1072\n","418/1072\n","419/1072\n","420/1072\n","421/1072\n","422/1072\n","423/1072\n","424/1072\n","425/1072\n","426/1072\n","427/1072\n","428/1072\n","429/1072\n","430/1072\n","431/1072\n","432/1072\n","433/1072\n","434/1072\n","435/1072\n","436/1072\n","437/1072\n","438/1072\n","439/1072\n","440/1072\n","441/1072\n","442/1072\n","443/1072\n","444/1072\n","445/1072\n","446/1072\n","447/1072\n","448/1072\n","449/1072\n","450/1072\n","451/1072\n","452/1072\n","453/1072\n","454/1072\n","455/1072\n","456/1072\n","457/1072\n","458/1072\n","459/1072\n","460/1072\n","461/1072\n","462/1072\n","463/1072\n","464/1072\n","465/1072\n","466/1072\n","467/1072\n","468/1072\n","469/1072\n","470/1072\n","471/1072\n","472/1072\n","473/1072\n","474/1072\n","475/1072\n","476/1072\n","477/1072\n","478/1072\n","479/1072\n","480/1072\n","481/1072\n","482/1072\n","483/1072\n","484/1072\n","485/1072\n","486/1072\n","487/1072\n","488/1072\n","489/1072\n","490/1072\n","491/1072\n","492/1072\n","493/1072\n","494/1072\n","495/1072\n","496/1072\n","497/1072\n","498/1072\n","499/1072\n","500/1072\n","501/1072\n","502/1072\n","503/1072\n","504/1072\n","505/1072\n","506/1072\n","507/1072\n","508/1072\n","509/1072\n","510/1072\n","511/1072\n","512/1072\n","513/1072\n","514/1072\n","515/1072\n","516/1072\n","517/1072\n","518/1072\n","519/1072\n","520/1072\n","521/1072\n","522/1072\n","523/1072\n","524/1072\n","525/1072\n","526/1072\n","527/1072\n","528/1072\n","529/1072\n","530/1072\n","531/1072\n","532/1072\n","533/1072\n","534/1072\n","535/1072\n","536/1072\n","537/1072\n","538/1072\n","539/1072\n","540/1072\n","541/1072\n","542/1072\n","543/1072\n","544/1072\n","545/1072\n","546/1072\n","547/1072\n","548/1072\n","549/1072\n","550/1072\n","551/1072\n","552/1072\n","553/1072\n","554/1072\n","555/1072\n","556/1072\n","557/1072\n","558/1072\n","559/1072\n","560/1072\n","561/1072\n","562/1072\n","563/1072\n","564/1072\n","565/1072\n","566/1072\n","567/1072\n","568/1072\n","569/1072\n","570/1072\n","571/1072\n","572/1072\n","573/1072\n","574/1072\n","575/1072\n","576/1072\n","577/1072\n","578/1072\n","579/1072\n","580/1072\n","581/1072\n","582/1072\n","583/1072\n","584/1072\n","585/1072\n","586/1072\n","587/1072\n","588/1072\n","589/1072\n","590/1072\n","591/1072\n","592/1072\n","593/1072\n","594/1072\n","595/1072\n","596/1072\n","597/1072\n","598/1072\n","599/1072\n","600/1072\n","601/1072\n","602/1072\n","603/1072\n","604/1072\n","605/1072\n","606/1072\n","607/1072\n","608/1072\n","609/1072\n","610/1072\n","611/1072\n","612/1072\n","613/1072\n","614/1072\n","615/1072\n","616/1072\n","617/1072\n","618/1072\n","619/1072\n","620/1072\n","621/1072\n","622/1072\n","623/1072\n","624/1072\n","625/1072\n","626/1072\n","627/1072\n","628/1072\n","629/1072\n","630/1072\n","631/1072\n","632/1072\n","633/1072\n","634/1072\n","635/1072\n","636/1072\n","637/1072\n","638/1072\n","639/1072\n","640/1072\n","641/1072\n","642/1072\n","643/1072\n","644/1072\n","645/1072\n","646/1072\n","647/1072\n","648/1072\n","649/1072\n","650/1072\n","651/1072\n","652/1072\n","653/1072\n","654/1072\n","655/1072\n","656/1072\n","657/1072\n","658/1072\n","659/1072\n","660/1072\n","661/1072\n","662/1072\n","663/1072\n","664/1072\n","665/1072\n","666/1072\n","667/1072\n","668/1072\n","669/1072\n","670/1072\n","671/1072\n","672/1072\n","673/1072\n","674/1072\n","675/1072\n","676/1072\n","677/1072\n","678/1072\n","679/1072\n","680/1072\n","681/1072\n","682/1072\n","683/1072\n","684/1072\n","685/1072\n","686/1072\n","687/1072\n","688/1072\n","689/1072\n","690/1072\n","691/1072\n","692/1072\n","693/1072\n","694/1072\n","695/1072\n","696/1072\n","697/1072\n","698/1072\n","699/1072\n","700/1072\n","701/1072\n","702/1072\n","703/1072\n","704/1072\n","705/1072\n","706/1072\n","707/1072\n","708/1072\n","709/1072\n","710/1072\n","711/1072\n","712/1072\n","713/1072\n","714/1072\n","715/1072\n","716/1072\n","717/1072\n","718/1072\n","719/1072\n","720/1072\n","721/1072\n","722/1072\n","723/1072\n","724/1072\n","725/1072\n","726/1072\n","727/1072\n","728/1072\n","729/1072\n","730/1072\n","731/1072\n","732/1072\n","733/1072\n","734/1072\n","735/1072\n","736/1072\n","737/1072\n","738/1072\n","739/1072\n","740/1072\n","741/1072\n","742/1072\n","743/1072\n","744/1072\n","745/1072\n","746/1072\n","747/1072\n","748/1072\n","749/1072\n","750/1072\n","751/1072\n","752/1072\n","753/1072\n","754/1072\n","755/1072\n","756/1072\n","757/1072\n","758/1072\n","759/1072\n","760/1072\n","761/1072\n","762/1072\n","763/1072\n","764/1072\n","765/1072\n","766/1072\n","767/1072\n","768/1072\n","769/1072\n","770/1072\n","771/1072\n","772/1072\n","773/1072\n","774/1072\n","775/1072\n","776/1072\n","777/1072\n","778/1072\n","779/1072\n","780/1072\n","781/1072\n","782/1072\n","783/1072\n","784/1072\n","785/1072\n","786/1072\n","787/1072\n","788/1072\n","789/1072\n","790/1072\n","791/1072\n","792/1072\n","793/1072\n","794/1072\n","795/1072\n","796/1072\n","797/1072\n","798/1072\n","799/1072\n","800/1072\n","801/1072\n","802/1072\n","803/1072\n","804/1072\n","805/1072\n","806/1072\n","807/1072\n","808/1072\n","809/1072\n","810/1072\n","811/1072\n","812/1072\n","813/1072\n","814/1072\n","815/1072\n","816/1072\n","817/1072\n","818/1072\n","819/1072\n","820/1072\n","821/1072\n","822/1072\n","823/1072\n","824/1072\n","825/1072\n","826/1072\n","827/1072\n","828/1072\n","829/1072\n","830/1072\n","831/1072\n","832/1072\n","833/1072\n","834/1072\n","835/1072\n","836/1072\n","837/1072\n","838/1072\n","839/1072\n","840/1072\n","841/1072\n","842/1072\n","843/1072\n","844/1072\n","845/1072\n","846/1072\n","847/1072\n","848/1072\n","849/1072\n","850/1072\n","851/1072\n","852/1072\n","853/1072\n","854/1072\n","855/1072\n","856/1072\n","857/1072\n","858/1072\n","859/1072\n","860/1072\n","861/1072\n","862/1072\n","863/1072\n","864/1072\n","865/1072\n","866/1072\n","867/1072\n","868/1072\n","869/1072\n","870/1072\n","871/1072\n","872/1072\n","873/1072\n","874/1072\n","875/1072\n","876/1072\n","877/1072\n","878/1072\n","879/1072\n","880/1072\n","881/1072\n","882/1072\n","883/1072\n","884/1072\n","885/1072\n","886/1072\n","887/1072\n","888/1072\n","889/1072\n","890/1072\n","891/1072\n","892/1072\n","893/1072\n","894/1072\n","895/1072\n","896/1072\n","897/1072\n","898/1072\n","899/1072\n","900/1072\n","901/1072\n","902/1072\n","903/1072\n","904/1072\n","905/1072\n","906/1072\n","907/1072\n","908/1072\n","909/1072\n","910/1072\n","911/1072\n","912/1072\n","913/1072\n","914/1072\n","915/1072\n","916/1072\n","917/1072\n","918/1072\n","919/1072\n","920/1072\n","921/1072\n","922/1072\n","923/1072\n","924/1072\n","925/1072\n","926/1072\n","927/1072\n","928/1072\n","929/1072\n","930/1072\n","931/1072\n","932/1072\n","933/1072\n","934/1072\n","935/1072\n","936/1072\n","937/1072\n","938/1072\n","939/1072\n","940/1072\n","941/1072\n","942/1072\n","943/1072\n","944/1072\n","945/1072\n","946/1072\n","947/1072\n","948/1072\n","949/1072\n","950/1072\n","951/1072\n","952/1072\n","953/1072\n","954/1072\n","955/1072\n","956/1072\n","957/1072\n","958/1072\n","959/1072\n","960/1072\n","961/1072\n","962/1072\n","963/1072\n","964/1072\n","965/1072\n","966/1072\n","967/1072\n","968/1072\n","969/1072\n","970/1072\n","971/1072\n","972/1072\n","973/1072\n","974/1072\n","975/1072\n","976/1072\n","977/1072\n","978/1072\n","979/1072\n","980/1072\n","981/1072\n","982/1072\n","983/1072\n","984/1072\n","985/1072\n","986/1072\n","987/1072\n","988/1072\n","989/1072\n","990/1072\n","991/1072\n","992/1072\n","993/1072\n","994/1072\n","995/1072\n","996/1072\n","997/1072\n","998/1072\n","999/1072\n","1000/1072\n","1001/1072\n","1002/1072\n","1003/1072\n","1004/1072\n","1005/1072\n","1006/1072\n","1007/1072\n","1008/1072\n","1009/1072\n","1010/1072\n","1011/1072\n","1012/1072\n","1013/1072\n","1014/1072\n","1015/1072\n","1016/1072\n","1017/1072\n","1018/1072\n","1019/1072\n","1020/1072\n","1021/1072\n","1022/1072\n","1023/1072\n","1024/1072\n","1025/1072\n","1026/1072\n","1027/1072\n","1028/1072\n","1029/1072\n","1030/1072\n","1031/1072\n","1032/1072\n","1033/1072\n","1034/1072\n","1035/1072\n","1036/1072\n","1037/1072\n","1038/1072\n","1039/1072\n","1040/1072\n","1041/1072\n","1042/1072\n","1043/1072\n","1044/1072\n","1045/1072\n","1046/1072\n","1047/1072\n","1048/1072\n","1049/1072\n","1050/1072\n","1051/1072\n","1052/1072\n","1053/1072\n","1054/1072\n","1055/1072\n","1056/1072\n","1057/1072\n","1058/1072\n","1059/1072\n","1060/1072\n","1061/1072\n","1062/1072\n","1063/1072\n","1064/1072\n","1065/1072\n","1066/1072\n","1067/1072\n","1068/1072\n","1069/1072\n","1070/1072\n","1071/1072\n","1072/1072\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score"],"metadata":{"id":"58zYZAKvRJCQ"},"id":"58zYZAKvRJCQ","execution_count":null,"outputs":[]},{"cell_type":"code","source":["accuracy_score(true, pred)"],"metadata":{"id":"HDwrxkwmItA_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650647838945,"user_tz":-480,"elapsed":7,"user":{"displayName":"Hpone Myat Khine","userId":"13391990417136282456"}},"outputId":"847b5945-18aa-40ed-ed31-235214c1c661"},"id":"HDwrxkwmItA_","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.5789212683410636"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["df = pd.DataFrame({'true':true,\n","                   'pred':pred})\n","\n","df.to_csv('/content/drive/MyDrive/Colab Notebooks/5153 weights/batchsize32_ViT_pretrained_test_pred.csv')"],"metadata":{"id":"gfA2KTi_C6Qf"},"id":"gfA2KTi_C6Qf","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"MbNNxc777pp1"},"id":"MbNNxc777pp1","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"paaBXuzj_62u"},"id":"paaBXuzj_62u","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"NSGRFtPt3j-v"},"id":"NSGRFtPt3j-v","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"KYLOtBro1HUo"},"id":"KYLOtBro1HUo","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3faPMbOImx1u"},"outputs":[],"source":[""],"id":"3faPMbOImx1u"},{"cell_type":"code","source":[""],"metadata":{"id":"EjgCOwi5vl8a"},"id":"EjgCOwi5vl8a","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"tNAWXk6Cs-Zw"},"id":"tNAWXk6Cs-Zw","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"ELnxY8QGrCq3"},"id":"ELnxY8QGrCq3","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"ngyt4uJEm1Sk"},"id":"ngyt4uJEm1Sk","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e5xtFBwW01Cj"},"outputs":[],"source":[""],"id":"e5xtFBwW01Cj"},{"cell_type":"code","execution_count":null,"metadata":{"id":"xXFX5poHfXsV"},"outputs":[],"source":[""],"id":"xXFX5poHfXsV"},{"cell_type":"code","execution_count":null,"metadata":{"id":"0YfLHtYSiuxR"},"outputs":[],"source":[""],"id":"0YfLHtYSiuxR"},{"cell_type":"code","execution_count":null,"metadata":{"id":"MR1BK4fESFv7"},"outputs":[],"source":[""],"id":"MR1BK4fESFv7"}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"ViT-B_32 - 5C5K.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}
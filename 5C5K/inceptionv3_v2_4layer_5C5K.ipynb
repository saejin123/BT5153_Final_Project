{"cells":[{"cell_type":"code","source":[""],"metadata":{"id":"ZXscx65xElBS","executionInfo":{"status":"ok","timestamp":1650685608532,"user_tz":-480,"elapsed":16,"user":{"displayName":"Hpone Myat Khine","userId":"13391990417136282456"}}},"id":"ZXscx65xElBS","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#from google.colab import drive\n","#drive.mount('/content/drive')"],"metadata":{"id":"f38by0m2Em-M","executionInfo":{"status":"ok","timestamp":1650685608534,"user_tz":-480,"elapsed":15,"user":{"displayName":"Hpone Myat Khine","userId":"13391990417136282456"}}},"id":"f38by0m2Em-M","execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":2,"id":"c1d47d3a","metadata":{"id":"c1d47d3a","executionInfo":{"status":"ok","timestamp":1650685611378,"user_tz":-480,"elapsed":2858,"user":{"displayName":"Hpone Myat Khine","userId":"13391990417136282456"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from random import sample\n","import matplotlib.pyplot as plt\n","from IPython.core.pylabtools import figsize\n","import glob\n","import cv2\n","\n","#from google.colab import drive\n","\n","import matplotlib.pyplot as plt\n","import keras as ks\n","from keras import layers\n","from keras.models import Sequential\n","import tensorflow as tf\n","from keras.backend import categorical_crossentropy\n","\n","import os\n","\n","from keras.applications.inception_v3 import preprocess_input\n","from keras.models import Sequential\n","from keras.models import Model\n","from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard\n","from keras import optimizers, losses, activations, models\n","from keras.layers import Convolution2D, Dense, Input, Flatten, Dropout, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D, Concatenate\n","from keras import applications\n","from tensorflow.keras.applications.inception_v3 import InceptionV3\n","from tensorflow.keras.optimizers import SGD"]},{"cell_type":"code","execution_count":3,"id":"c83a347f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c83a347f","executionInfo":{"status":"ok","timestamp":1650685622050,"user_tz":-480,"elapsed":10713,"user":{"displayName":"Hpone Myat Khine","userId":"13391990417136282456"}},"outputId":"5d3245ef-4379-4086-e7c8-102b947d9ff8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 25000 files belonging to 5 classes.\n","Found 27447 files belonging to 5 classes.\n","Found 34281 files belonging to 5 classes.\n"]}],"source":["image_size = (250, 250)\n","batch_size = 32\n","\n"," \n","train_directory = '/content/drive/MyDrive/Colab Notebooks/BT5153 Applied ML Project /5 classes 5k each for train/Training'\n","val_directory = '/content/drive/MyDrive/Colab Notebooks/BT5153 Applied ML Project /5 classes 5k each for train/Validation'\n","test_directory = '/content/drive/MyDrive/Colab Notebooks/BT5153 Applied ML Project /5 classes 5k each for train/Testing'\n","\n","train_ds = tf.keras.preprocessing.image_dataset_from_directory(train_directory, label_mode='categorical', image_size=image_size, batch_size=batch_size)\n","val_ds = tf.keras.preprocessing.image_dataset_from_directory(val_directory, label_mode='categorical', image_size=image_size, batch_size=batch_size)\n","test_ds = tf.keras.preprocessing.image_dataset_from_directory(test_directory, label_mode='categorical', image_size=image_size, batch_size=batch_size)\n","\n","train_ds = train_ds.prefetch(buffer_size=32)\n","val_ds = val_ds.prefetch(buffer_size=32)\n","test_ds = test_ds.prefetch(buffer_size=32)"]},{"cell_type":"code","execution_count":4,"id":"d99d2484","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d99d2484","executionInfo":{"status":"ok","timestamp":1650685624979,"user_tz":-480,"elapsed":2936,"user":{"displayName":"Hpone Myat Khine","userId":"13391990417136282456"}},"outputId":"dd4a1ef4-8b7d-4c10-f1fb-377d882030c8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," inception_v3 (Functional)   (None, 6, 6, 2048)        21802784  \n","                                                                 \n"," rescaling (Rescaling)       (None, 6, 6, 2048)        0         \n","                                                                 \n"," conv2d_94 (Conv2D)          (None, 3, 3, 32)          589856    \n","                                                                 \n"," batch_normalization_94 (Bat  (None, 3, 3, 32)         128       \n"," chNormalization)                                                \n","                                                                 \n"," activation_94 (Activation)  (None, 3, 3, 32)          0         \n","                                                                 \n"," global_average_pooling2d (G  (None, 32)               0         \n"," lobalAveragePooling2D)                                          \n","                                                                 \n"," dropout (Dropout)           (None, 32)                0         \n","                                                                 \n"," dense (Dense)               (None, 5)                 165       \n","                                                                 \n","=================================================================\n","Total params: 22,392,933\n","Trainable params: 22,358,437\n","Non-trainable params: 34,496\n","_________________________________________________________________\n"]}],"source":["input_shape = (250, 250)\n","nclass = 5\n","\n","# create the base pre-trained model \n","base_model = InceptionV3(weights='imagenet', \n","                                include_top=False, \n","                                input_shape= (input_shape+(3,)))\n","\n","# to freeze the base model \n","# https://keras.io/guides/transfer_learning/\n","#base_model.trainable = False\n","\n","# to add layers to the model \n","add_model = Sequential()\n","add_model.add(base_model)\n","add_model.add(layers.Rescaling(1.0/255))\n","add_model.add(layers.Conv2D(32, 3, strides=2, padding='same'))\n","add_model.add(layers.BatchNormalization())\n","add_model.add(layers.Activation('relu'))\n","add_model.add(GlobalAveragePooling2D()) # to add a global spatial average pooling layer \n","add_model.add(Dropout(0.5))\n","add_model.add(Dense(nclass, activation='softmax')) #there are 5 classes \n","\n","# compile the model (should be done *after* setting layers to non-trainable)\n","model = add_model\n","model.compile(optimizer='adam', loss='CategoricalCrossentropy', metrics=['accuracy'])\n","model.summary()"]},{"cell_type":"code","execution_count":5,"id":"df192d84","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"df192d84","executionInfo":{"status":"ok","timestamp":1650690397667,"user_tz":-480,"elapsed":4772806,"user":{"displayName":"Hpone Myat Khine","userId":"13391990417136282456"}},"outputId":"276226ff-be17-4b37-d6c4-5d6687114d70"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/1000\n","782/782 [==============================] - ETA: 0s - loss: 0.8807 - accuracy: 0.6966\n","Epoch 1: val_loss improved from inf to 1.64368, saving model to /content/drive/MyDrive/Colab Notebooks/5153 weights/inceptionv3_V2_2_5C5K_1.h5\n","782/782 [==============================] - 341s 417ms/step - loss: 0.8807 - accuracy: 0.6966 - val_loss: 1.6437 - val_accuracy: 0.4457\n","Epoch 2/1000\n","782/782 [==============================] - ETA: 0s - loss: 0.6258 - accuracy: 0.7969\n","Epoch 2: val_loss improved from 1.64368 to 0.78223, saving model to /content/drive/MyDrive/Colab Notebooks/5153 weights/inceptionv3_V2_2_5C5K_2.h5\n","782/782 [==============================] - 318s 406ms/step - loss: 0.6258 - accuracy: 0.7969 - val_loss: 0.7822 - val_accuracy: 0.7083\n","Epoch 3/1000\n","782/782 [==============================] - ETA: 0s - loss: 0.5273 - accuracy: 0.8331\n","Epoch 3: val_loss did not improve from 0.78223\n","782/782 [==============================] - 317s 405ms/step - loss: 0.5273 - accuracy: 0.8331 - val_loss: 2.9656 - val_accuracy: 0.3955\n","Epoch 4/1000\n","782/782 [==============================] - ETA: 0s - loss: 0.4640 - accuracy: 0.8540\n","Epoch 4: val_loss did not improve from 0.78223\n","782/782 [==============================] - 316s 404ms/step - loss: 0.4640 - accuracy: 0.8540 - val_loss: 2.0928 - val_accuracy: 0.4883\n","Epoch 5/1000\n","782/782 [==============================] - ETA: 0s - loss: 0.4199 - accuracy: 0.8703\n","Epoch 5: val_loss improved from 0.78223 to 0.74619, saving model to /content/drive/MyDrive/Colab Notebooks/5153 weights/inceptionv3_V2_2_5C5K_5.h5\n","782/782 [==============================] - 317s 406ms/step - loss: 0.4199 - accuracy: 0.8703 - val_loss: 0.7462 - val_accuracy: 0.7430\n","Epoch 6/1000\n","782/782 [==============================] - ETA: 0s - loss: 0.3730 - accuracy: 0.8846\n","Epoch 6: val_loss did not improve from 0.74619\n","782/782 [==============================] - 316s 404ms/step - loss: 0.3730 - accuracy: 0.8846 - val_loss: 1.6223 - val_accuracy: 0.6000\n","Epoch 7/1000\n","782/782 [==============================] - ETA: 0s - loss: 0.3336 - accuracy: 0.8955\n","Epoch 7: val_loss did not improve from 0.74619\n","782/782 [==============================] - 315s 403ms/step - loss: 0.3336 - accuracy: 0.8955 - val_loss: 1.4715 - val_accuracy: 0.6291\n","Epoch 8/1000\n","782/782 [==============================] - ETA: 0s - loss: 0.3081 - accuracy: 0.9063\n","Epoch 8: val_loss did not improve from 0.74619\n","782/782 [==============================] - 315s 403ms/step - loss: 0.3081 - accuracy: 0.9063 - val_loss: 0.8398 - val_accuracy: 0.7548\n","Epoch 9/1000\n","782/782 [==============================] - ETA: 0s - loss: 0.2638 - accuracy: 0.9201\n","Epoch 9: val_loss did not improve from 0.74619\n","782/782 [==============================] - 315s 403ms/step - loss: 0.2638 - accuracy: 0.9201 - val_loss: 1.0638 - val_accuracy: 0.6970\n","Epoch 10/1000\n","782/782 [==============================] - ETA: 0s - loss: 0.2394 - accuracy: 0.9278\n","Epoch 10: val_loss did not improve from 0.74619\n","782/782 [==============================] - 315s 403ms/step - loss: 0.2394 - accuracy: 0.9278 - val_loss: 0.8508 - val_accuracy: 0.7622\n","Epoch 11/1000\n","782/782 [==============================] - ETA: 0s - loss: 0.2099 - accuracy: 0.9359\n","Epoch 11: val_loss did not improve from 0.74619\n","782/782 [==============================] - 316s 403ms/step - loss: 0.2099 - accuracy: 0.9359 - val_loss: 1.3137 - val_accuracy: 0.6978\n","Epoch 12/1000\n","782/782 [==============================] - ETA: 0s - loss: 0.1901 - accuracy: 0.9408\n","Epoch 12: val_loss did not improve from 0.74619\n","782/782 [==============================] - 315s 403ms/step - loss: 0.1901 - accuracy: 0.9408 - val_loss: 0.9085 - val_accuracy: 0.7695\n","Epoch 13/1000\n","782/782 [==============================] - ETA: 0s - loss: 0.1594 - accuracy: 0.9509\n","Epoch 13: val_loss did not improve from 0.74619\n","782/782 [==============================] - 316s 403ms/step - loss: 0.1594 - accuracy: 0.9509 - val_loss: 0.8511 - val_accuracy: 0.7832\n","Epoch 14/1000\n","782/782 [==============================] - ETA: 0s - loss: 0.1604 - accuracy: 0.9509\n","Epoch 14: val_loss did not improve from 0.74619\n","782/782 [==============================] - 316s 404ms/step - loss: 0.1604 - accuracy: 0.9509 - val_loss: 2.1915 - val_accuracy: 0.6091\n","Epoch 15/1000\n","782/782 [==============================] - ETA: 0s - loss: 0.1358 - accuracy: 0.9578\n","Epoch 15: val_loss did not improve from 0.74619\n","782/782 [==============================] - 316s 403ms/step - loss: 0.1358 - accuracy: 0.9578 - val_loss: 1.0975 - val_accuracy: 0.7176\n"]}],"source":["file_path=\"/content/drive/MyDrive/Colab Notebooks/5153 weights/inceptionv3_V2_2_5C5K_{epoch}.h5\"\n","epochs = 1000\n","\n","checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","\n","early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=10)\n","\n","callbacks_list = [checkpoint, early] #early\n","\n","history = model.fit(train_ds, epochs=epochs, validation_data=val_ds, callbacks=callbacks_list)"]},{"cell_type":"code","source":[""],"metadata":{"id":"PwGfX7VePkfS","executionInfo":{"status":"ok","timestamp":1650690397670,"user_tz":-480,"elapsed":46,"user":{"displayName":"Hpone Myat Khine","userId":"13391990417136282456"}}},"id":"PwGfX7VePkfS","execution_count":5,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"pYY4eMc4QgH8","executionInfo":{"status":"ok","timestamp":1650690397675,"user_tz":-480,"elapsed":30,"user":{"displayName":"Hpone Myat Khine","userId":"13391990417136282456"}}},"id":"pYY4eMc4QgH8","execution_count":5,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score"],"metadata":{"id":"tqZSo7iuNoZ5","executionInfo":{"status":"ok","timestamp":1650691332848,"user_tz":-480,"elapsed":377,"user":{"displayName":"Hpone Myat Khine","userId":"13391990417136282456"}}},"id":"tqZSo7iuNoZ5","execution_count":7,"outputs":[]},{"cell_type":"code","execution_count":8,"id":"3bb67ed7","metadata":{"id":"3bb67ed7","executionInfo":{"status":"ok","timestamp":1650691357102,"user_tz":-480,"elapsed":21619,"user":{"displayName":"Hpone Myat Khine","userId":"13391990417136282456"}}},"outputs":[],"source":["##load model at epoch 5 --> best weight (alternatively, set restore_best_weights = True when fitt)\n","new_model = ks.models.load_model('/content/drive/MyDrive/Colab Notebooks/5153 weights/inceptionv3_V2_2_5C5K_5.h5')\n"]},{"cell_type":"code","source":["true = []\n","pred = []\n","batches = int(np.ceil(34281/32))\n","i = 1\n","for image, label in test_ds:\n","  pred = np.concatenate([pred, \n","                         np.argmax(new_model.predict(image), axis=-1)]) #use new_model\n","  true = np.concatenate([true,\n","                         np.argmax(label.numpy(), axis=-1)])\n","  print('{}/{}'.format(i, batches))\n","  i += 1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s-zslJP-1H_n","executionInfo":{"status":"ok","timestamp":1650695179107,"user_tz":-480,"elapsed":3818609,"user":{"displayName":"Hpone Myat Khine","userId":"13391990417136282456"}},"outputId":"134c8cfb-bb10-4c6c-8cc0-c4ce7b34161b"},"id":"s-zslJP-1H_n","execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1072\n","2/1072\n","3/1072\n","4/1072\n","5/1072\n","6/1072\n","7/1072\n","8/1072\n","9/1072\n","10/1072\n","11/1072\n","12/1072\n","13/1072\n","14/1072\n","15/1072\n","16/1072\n","17/1072\n","18/1072\n","19/1072\n","20/1072\n","21/1072\n","22/1072\n","23/1072\n","24/1072\n","25/1072\n","26/1072\n","27/1072\n","28/1072\n","29/1072\n","30/1072\n","31/1072\n","32/1072\n","33/1072\n","34/1072\n","35/1072\n","36/1072\n","37/1072\n","38/1072\n","39/1072\n","40/1072\n","41/1072\n","42/1072\n","43/1072\n","44/1072\n","45/1072\n","46/1072\n","47/1072\n","48/1072\n","49/1072\n","50/1072\n","51/1072\n","52/1072\n","53/1072\n","54/1072\n","55/1072\n","56/1072\n","57/1072\n","58/1072\n","59/1072\n","60/1072\n","61/1072\n","62/1072\n","63/1072\n","64/1072\n","65/1072\n","66/1072\n","67/1072\n","68/1072\n","69/1072\n","70/1072\n","71/1072\n","72/1072\n","73/1072\n","74/1072\n","75/1072\n","76/1072\n","77/1072\n","78/1072\n","79/1072\n","80/1072\n","81/1072\n","82/1072\n","83/1072\n","84/1072\n","85/1072\n","86/1072\n","87/1072\n","88/1072\n","89/1072\n","90/1072\n","91/1072\n","92/1072\n","93/1072\n","94/1072\n","95/1072\n","96/1072\n","97/1072\n","98/1072\n","99/1072\n","100/1072\n","101/1072\n","102/1072\n","103/1072\n","104/1072\n","105/1072\n","106/1072\n","107/1072\n","108/1072\n","109/1072\n","110/1072\n","111/1072\n","112/1072\n","113/1072\n","114/1072\n","115/1072\n","116/1072\n","117/1072\n","118/1072\n","119/1072\n","120/1072\n","121/1072\n","122/1072\n","123/1072\n","124/1072\n","125/1072\n","126/1072\n","127/1072\n","128/1072\n","129/1072\n","130/1072\n","131/1072\n","132/1072\n","133/1072\n","134/1072\n","135/1072\n","136/1072\n","137/1072\n","138/1072\n","139/1072\n","140/1072\n","141/1072\n","142/1072\n","143/1072\n","144/1072\n","145/1072\n","146/1072\n","147/1072\n","148/1072\n","149/1072\n","150/1072\n","151/1072\n","152/1072\n","153/1072\n","154/1072\n","155/1072\n","156/1072\n","157/1072\n","158/1072\n","159/1072\n","160/1072\n","161/1072\n","162/1072\n","163/1072\n","164/1072\n","165/1072\n","166/1072\n","167/1072\n","168/1072\n","169/1072\n","170/1072\n","171/1072\n","172/1072\n","173/1072\n","174/1072\n","175/1072\n","176/1072\n","177/1072\n","178/1072\n","179/1072\n","180/1072\n","181/1072\n","182/1072\n","183/1072\n","184/1072\n","185/1072\n","186/1072\n","187/1072\n","188/1072\n","189/1072\n","190/1072\n","191/1072\n","192/1072\n","193/1072\n","194/1072\n","195/1072\n","196/1072\n","197/1072\n","198/1072\n","199/1072\n","200/1072\n","201/1072\n","202/1072\n","203/1072\n","204/1072\n","205/1072\n","206/1072\n","207/1072\n","208/1072\n","209/1072\n","210/1072\n","211/1072\n","212/1072\n","213/1072\n","214/1072\n","215/1072\n","216/1072\n","217/1072\n","218/1072\n","219/1072\n","220/1072\n","221/1072\n","222/1072\n","223/1072\n","224/1072\n","225/1072\n","226/1072\n","227/1072\n","228/1072\n","229/1072\n","230/1072\n","231/1072\n","232/1072\n","233/1072\n","234/1072\n","235/1072\n","236/1072\n","237/1072\n","238/1072\n","239/1072\n","240/1072\n","241/1072\n","242/1072\n","243/1072\n","244/1072\n","245/1072\n","246/1072\n","247/1072\n","248/1072\n","249/1072\n","250/1072\n","251/1072\n","252/1072\n","253/1072\n","254/1072\n","255/1072\n","256/1072\n","257/1072\n","258/1072\n","259/1072\n","260/1072\n","261/1072\n","262/1072\n","263/1072\n","264/1072\n","265/1072\n","266/1072\n","267/1072\n","268/1072\n","269/1072\n","270/1072\n","271/1072\n","272/1072\n","273/1072\n","274/1072\n","275/1072\n","276/1072\n","277/1072\n","278/1072\n","279/1072\n","280/1072\n","281/1072\n","282/1072\n","283/1072\n","284/1072\n","285/1072\n","286/1072\n","287/1072\n","288/1072\n","289/1072\n","290/1072\n","291/1072\n","292/1072\n","293/1072\n","294/1072\n","295/1072\n","296/1072\n","297/1072\n","298/1072\n","299/1072\n","300/1072\n","301/1072\n","302/1072\n","303/1072\n","304/1072\n","305/1072\n","306/1072\n","307/1072\n","308/1072\n","309/1072\n","310/1072\n","311/1072\n","312/1072\n","313/1072\n","314/1072\n","315/1072\n","316/1072\n","317/1072\n","318/1072\n","319/1072\n","320/1072\n","321/1072\n","322/1072\n","323/1072\n","324/1072\n","325/1072\n","326/1072\n","327/1072\n","328/1072\n","329/1072\n","330/1072\n","331/1072\n","332/1072\n","333/1072\n","334/1072\n","335/1072\n","336/1072\n","337/1072\n","338/1072\n","339/1072\n","340/1072\n","341/1072\n","342/1072\n","343/1072\n","344/1072\n","345/1072\n","346/1072\n","347/1072\n","348/1072\n","349/1072\n","350/1072\n","351/1072\n","352/1072\n","353/1072\n","354/1072\n","355/1072\n","356/1072\n","357/1072\n","358/1072\n","359/1072\n","360/1072\n","361/1072\n","362/1072\n","363/1072\n","364/1072\n","365/1072\n","366/1072\n","367/1072\n","368/1072\n","369/1072\n","370/1072\n","371/1072\n","372/1072\n","373/1072\n","374/1072\n","375/1072\n","376/1072\n","377/1072\n","378/1072\n","379/1072\n","380/1072\n","381/1072\n","382/1072\n","383/1072\n","384/1072\n","385/1072\n","386/1072\n","387/1072\n","388/1072\n","389/1072\n","390/1072\n","391/1072\n","392/1072\n","393/1072\n","394/1072\n","395/1072\n","396/1072\n","397/1072\n","398/1072\n","399/1072\n","400/1072\n","401/1072\n","402/1072\n","403/1072\n","404/1072\n","405/1072\n","406/1072\n","407/1072\n","408/1072\n","409/1072\n","410/1072\n","411/1072\n","412/1072\n","413/1072\n","414/1072\n","415/1072\n","416/1072\n","417/1072\n","418/1072\n","419/1072\n","420/1072\n","421/1072\n","422/1072\n","423/1072\n","424/1072\n","425/1072\n","426/1072\n","427/1072\n","428/1072\n","429/1072\n","430/1072\n","431/1072\n","432/1072\n","433/1072\n","434/1072\n","435/1072\n","436/1072\n","437/1072\n","438/1072\n","439/1072\n","440/1072\n","441/1072\n","442/1072\n","443/1072\n","444/1072\n","445/1072\n","446/1072\n","447/1072\n","448/1072\n","449/1072\n","450/1072\n","451/1072\n","452/1072\n","453/1072\n","454/1072\n","455/1072\n","456/1072\n","457/1072\n","458/1072\n","459/1072\n","460/1072\n","461/1072\n","462/1072\n","463/1072\n","464/1072\n","465/1072\n","466/1072\n","467/1072\n","468/1072\n","469/1072\n","470/1072\n","471/1072\n","472/1072\n","473/1072\n","474/1072\n","475/1072\n","476/1072\n","477/1072\n","478/1072\n","479/1072\n","480/1072\n","481/1072\n","482/1072\n","483/1072\n","484/1072\n","485/1072\n","486/1072\n","487/1072\n","488/1072\n","489/1072\n","490/1072\n","491/1072\n","492/1072\n","493/1072\n","494/1072\n","495/1072\n","496/1072\n","497/1072\n","498/1072\n","499/1072\n","500/1072\n","501/1072\n","502/1072\n","503/1072\n","504/1072\n","505/1072\n","506/1072\n","507/1072\n","508/1072\n","509/1072\n","510/1072\n","511/1072\n","512/1072\n","513/1072\n","514/1072\n","515/1072\n","516/1072\n","517/1072\n","518/1072\n","519/1072\n","520/1072\n","521/1072\n","522/1072\n","523/1072\n","524/1072\n","525/1072\n","526/1072\n","527/1072\n","528/1072\n","529/1072\n","530/1072\n","531/1072\n","532/1072\n","533/1072\n","534/1072\n","535/1072\n","536/1072\n","537/1072\n","538/1072\n","539/1072\n","540/1072\n","541/1072\n","542/1072\n","543/1072\n","544/1072\n","545/1072\n","546/1072\n","547/1072\n","548/1072\n","549/1072\n","550/1072\n","551/1072\n","552/1072\n","553/1072\n","554/1072\n","555/1072\n","556/1072\n","557/1072\n","558/1072\n","559/1072\n","560/1072\n","561/1072\n","562/1072\n","563/1072\n","564/1072\n","565/1072\n","566/1072\n","567/1072\n","568/1072\n","569/1072\n","570/1072\n","571/1072\n","572/1072\n","573/1072\n","574/1072\n","575/1072\n","576/1072\n","577/1072\n","578/1072\n","579/1072\n","580/1072\n","581/1072\n","582/1072\n","583/1072\n","584/1072\n","585/1072\n","586/1072\n","587/1072\n","588/1072\n","589/1072\n","590/1072\n","591/1072\n","592/1072\n","593/1072\n","594/1072\n","595/1072\n","596/1072\n","597/1072\n","598/1072\n","599/1072\n","600/1072\n","601/1072\n","602/1072\n","603/1072\n","604/1072\n","605/1072\n","606/1072\n","607/1072\n","608/1072\n","609/1072\n","610/1072\n","611/1072\n","612/1072\n","613/1072\n","614/1072\n","615/1072\n","616/1072\n","617/1072\n","618/1072\n","619/1072\n","620/1072\n","621/1072\n","622/1072\n","623/1072\n","624/1072\n","625/1072\n","626/1072\n","627/1072\n","628/1072\n","629/1072\n","630/1072\n","631/1072\n","632/1072\n","633/1072\n","634/1072\n","635/1072\n","636/1072\n","637/1072\n","638/1072\n","639/1072\n","640/1072\n","641/1072\n","642/1072\n","643/1072\n","644/1072\n","645/1072\n","646/1072\n","647/1072\n","648/1072\n","649/1072\n","650/1072\n","651/1072\n","652/1072\n","653/1072\n","654/1072\n","655/1072\n","656/1072\n","657/1072\n","658/1072\n","659/1072\n","660/1072\n","661/1072\n","662/1072\n","663/1072\n","664/1072\n","665/1072\n","666/1072\n","667/1072\n","668/1072\n","669/1072\n","670/1072\n","671/1072\n","672/1072\n","673/1072\n","674/1072\n","675/1072\n","676/1072\n","677/1072\n","678/1072\n","679/1072\n","680/1072\n","681/1072\n","682/1072\n","683/1072\n","684/1072\n","685/1072\n","686/1072\n","687/1072\n","688/1072\n","689/1072\n","690/1072\n","691/1072\n","692/1072\n","693/1072\n","694/1072\n","695/1072\n","696/1072\n","697/1072\n","698/1072\n","699/1072\n","700/1072\n","701/1072\n","702/1072\n","703/1072\n","704/1072\n","705/1072\n","706/1072\n","707/1072\n","708/1072\n","709/1072\n","710/1072\n","711/1072\n","712/1072\n","713/1072\n","714/1072\n","715/1072\n","716/1072\n","717/1072\n","718/1072\n","719/1072\n","720/1072\n","721/1072\n","722/1072\n","723/1072\n","724/1072\n","725/1072\n","726/1072\n","727/1072\n","728/1072\n","729/1072\n","730/1072\n","731/1072\n","732/1072\n","733/1072\n","734/1072\n","735/1072\n","736/1072\n","737/1072\n","738/1072\n","739/1072\n","740/1072\n","741/1072\n","742/1072\n","743/1072\n","744/1072\n","745/1072\n","746/1072\n","747/1072\n","748/1072\n","749/1072\n","750/1072\n","751/1072\n","752/1072\n","753/1072\n","754/1072\n","755/1072\n","756/1072\n","757/1072\n","758/1072\n","759/1072\n","760/1072\n","761/1072\n","762/1072\n","763/1072\n","764/1072\n","765/1072\n","766/1072\n","767/1072\n","768/1072\n","769/1072\n","770/1072\n","771/1072\n","772/1072\n","773/1072\n","774/1072\n","775/1072\n","776/1072\n","777/1072\n","778/1072\n","779/1072\n","780/1072\n","781/1072\n","782/1072\n","783/1072\n","784/1072\n","785/1072\n","786/1072\n","787/1072\n","788/1072\n","789/1072\n","790/1072\n","791/1072\n","792/1072\n","793/1072\n","794/1072\n","795/1072\n","796/1072\n","797/1072\n","798/1072\n","799/1072\n","800/1072\n","801/1072\n","802/1072\n","803/1072\n","804/1072\n","805/1072\n","806/1072\n","807/1072\n","808/1072\n","809/1072\n","810/1072\n","811/1072\n","812/1072\n","813/1072\n","814/1072\n","815/1072\n","816/1072\n","817/1072\n","818/1072\n","819/1072\n","820/1072\n","821/1072\n","822/1072\n","823/1072\n","824/1072\n","825/1072\n","826/1072\n","827/1072\n","828/1072\n","829/1072\n","830/1072\n","831/1072\n","832/1072\n","833/1072\n","834/1072\n","835/1072\n","836/1072\n","837/1072\n","838/1072\n","839/1072\n","840/1072\n","841/1072\n","842/1072\n","843/1072\n","844/1072\n","845/1072\n","846/1072\n","847/1072\n","848/1072\n","849/1072\n","850/1072\n","851/1072\n","852/1072\n","853/1072\n","854/1072\n","855/1072\n","856/1072\n","857/1072\n","858/1072\n","859/1072\n","860/1072\n","861/1072\n","862/1072\n","863/1072\n","864/1072\n","865/1072\n","866/1072\n","867/1072\n","868/1072\n","869/1072\n","870/1072\n","871/1072\n","872/1072\n","873/1072\n","874/1072\n","875/1072\n","876/1072\n","877/1072\n","878/1072\n","879/1072\n","880/1072\n","881/1072\n","882/1072\n","883/1072\n","884/1072\n","885/1072\n","886/1072\n","887/1072\n","888/1072\n","889/1072\n","890/1072\n","891/1072\n","892/1072\n","893/1072\n","894/1072\n","895/1072\n","896/1072\n","897/1072\n","898/1072\n","899/1072\n","900/1072\n","901/1072\n","902/1072\n","903/1072\n","904/1072\n","905/1072\n","906/1072\n","907/1072\n","908/1072\n","909/1072\n","910/1072\n","911/1072\n","912/1072\n","913/1072\n","914/1072\n","915/1072\n","916/1072\n","917/1072\n","918/1072\n","919/1072\n","920/1072\n","921/1072\n","922/1072\n","923/1072\n","924/1072\n","925/1072\n","926/1072\n","927/1072\n","928/1072\n","929/1072\n","930/1072\n","931/1072\n","932/1072\n","933/1072\n","934/1072\n","935/1072\n","936/1072\n","937/1072\n","938/1072\n","939/1072\n","940/1072\n","941/1072\n","942/1072\n","943/1072\n","944/1072\n","945/1072\n","946/1072\n","947/1072\n","948/1072\n","949/1072\n","950/1072\n","951/1072\n","952/1072\n","953/1072\n","954/1072\n","955/1072\n","956/1072\n","957/1072\n","958/1072\n","959/1072\n","960/1072\n","961/1072\n","962/1072\n","963/1072\n","964/1072\n","965/1072\n","966/1072\n","967/1072\n","968/1072\n","969/1072\n","970/1072\n","971/1072\n","972/1072\n","973/1072\n","974/1072\n","975/1072\n","976/1072\n","977/1072\n","978/1072\n","979/1072\n","980/1072\n","981/1072\n","982/1072\n","983/1072\n","984/1072\n","985/1072\n","986/1072\n","987/1072\n","988/1072\n","989/1072\n","990/1072\n","991/1072\n","992/1072\n","993/1072\n","994/1072\n","995/1072\n","996/1072\n","997/1072\n","998/1072\n","999/1072\n","1000/1072\n","1001/1072\n","1002/1072\n","1003/1072\n","1004/1072\n","1005/1072\n","1006/1072\n","1007/1072\n","1008/1072\n","1009/1072\n","1010/1072\n","1011/1072\n","1012/1072\n","1013/1072\n","1014/1072\n","1015/1072\n","1016/1072\n","1017/1072\n","1018/1072\n","1019/1072\n","1020/1072\n","1021/1072\n","1022/1072\n","1023/1072\n","1024/1072\n","1025/1072\n","1026/1072\n","1027/1072\n","1028/1072\n","1029/1072\n","1030/1072\n","1031/1072\n","1032/1072\n","1033/1072\n","1034/1072\n","1035/1072\n","1036/1072\n","1037/1072\n","1038/1072\n","1039/1072\n","1040/1072\n","1041/1072\n","1042/1072\n","1043/1072\n","1044/1072\n","1045/1072\n","1046/1072\n","1047/1072\n","1048/1072\n","1049/1072\n","1050/1072\n","1051/1072\n","1052/1072\n","1053/1072\n","1054/1072\n","1055/1072\n","1056/1072\n","1057/1072\n","1058/1072\n","1059/1072\n","1060/1072\n","1061/1072\n","1062/1072\n","1063/1072\n","1064/1072\n","1065/1072\n","1066/1072\n","1067/1072\n","1068/1072\n","1069/1072\n","1070/1072\n","1071/1072\n","1072/1072\n"]}]},{"cell_type":"code","source":["accuracy_score(true, pred)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AoHoKW3E2-XK","executionInfo":{"status":"ok","timestamp":1650695179109,"user_tz":-480,"elapsed":48,"user":{"displayName":"Hpone Myat Khine","userId":"13391990417136282456"}},"outputId":"6d060718-f89e-49b5-a826-d5df77a01dbd"},"id":"AoHoKW3E2-XK","execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7425687698725242"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["model_results = pd.DataFrame({'true':true,\n","                              'pred':pred})\n","\n","model_results.to_csv('/content/drive/MyDrive/Colab Notebooks/5153 weights/inceptionv3_V2_2_test_pred.csv')"],"metadata":{"id":"KK1Zruen2HU3","executionInfo":{"status":"ok","timestamp":1650695183563,"user_tz":-480,"elapsed":434,"user":{"displayName":"Hpone Myat Khine","userId":"13391990417136282456"}}},"id":"KK1Zruen2HU3","execution_count":11,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"ihmkwzcj_bWf"},"id":"ihmkwzcj_bWf","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"ahL510unAaNC"},"id":"ahL510unAaNC","execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"-6sKcgmDC4Os"},"id":"-6sKcgmDC4Os","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"name":"inceptionv3_v2-Copy1.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}